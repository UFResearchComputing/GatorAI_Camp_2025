{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/UFResearchComputing/gatorAI_summer_camp_2024/blob/main/01_full_of_emotion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><img src=\"images/gator_ai_camp_2024_logo_200.png\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KhpJb5VI79P"
   },
   "source": [
    "# Gator AI Summer Camp 2025\n",
    "\n",
    "In this notebook, we're going to use Python to create a deep learning model that can take images of faces and output the emotion being expressed.\n",
    "\n",
    "The dataset we're going to use is the FER-2013 dataset, which contains 35,887 grayscale images of faces. Each image is 48x48 pixels and is labeled with one of seven emotions: anger, disgust, fear, happiness, sadness, surprise, or neutral. The dataset and more information can be found [on Kaggle](https://www.kaggle.com/datasets/msambare/fer2013/data).\n",
    "\n",
    "**Note:** One issue with the dataset is that it has relatively few images in the disgust category, so we drop that category for this exercise.\n",
    "\n",
    "To build our model, we'll use the Keras deep learning library, which provides a high-level interface for building and training neural networks. We'll start by loading the dataset and exploring the images, then we'll build and train a convolutional neural network (CNN) to classify the emotions in the images.\n",
    "\n",
    "**Before you get started, make sure to select a Runtime with a GPU!** <img src='images/colab_change_runtime_type.png' align='right' width='50%' alt='Image of the Runtime menu options in Google Colab'>\n",
    "* Go to the **\"Runtime\"** menu\n",
    "* Select **\"Change runtime type\"**\n",
    "* Select **\"T4 GPU\"** and click **\"Save\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Learning Objectives & What You'll Build\n",
    "\n",
    "## üß† What is Computer Vision?\n",
    "Computer Vision is a field of AI that teaches computers to \"see\" and understand images, just like humans do! In this notebook, you'll build a system that can look at a person's face and automatically detect their emotion.\n",
    "\n",
    "## üéÆ Real-World Application: Emotion-Aware Gaming\n",
    "The emotion recognition model you'll create will be integrated into our adventure game, allowing Non-Player Characters (NPCs) to respond differently based on your facial expressions. Imagine:\n",
    "- **Sad expression** ‚Üí NPCs offer comfort and help\n",
    "- **Happy expression** ‚Üí NPCs share in your joy and give bonuses  \n",
    "- **Angry expression** ‚Üí NPCs try to calm you down\n",
    "- **Surprised expression** ‚Üí NPCs react to your amazement\n",
    "\n",
    "## üìö What You'll Learn Today\n",
    "\n",
    "### üî¨ **Computer Vision Concepts**\n",
    "- How computers \"see\" and process images\n",
    "- What makes facial expressions recognizable\n",
    "- Image preprocessing and data augmentation\n",
    "\n",
    "### üß† **Deep Learning Fundamentals**\n",
    "- **Convolutional Neural Networks (CNNs)** - the AI architecture that powers image recognition\n",
    "- **Training Process** - how AI learns from thousands of examples\n",
    "- **Model Evaluation** - measuring how well our AI performs\n",
    "\n",
    "### üõ†Ô∏è **Practical Skills**\n",
    "- Using **PyTorch Lightning** for efficient deep learning\n",
    "- Working with real-world datasets (FER-2013 emotion dataset)\n",
    "- Visualizing model performance and debugging\n",
    "- Saving and loading trained models for deployment\n",
    "\n",
    "### üéÆ **Game Integration**\n",
    "- Loading pre-trained models in applications\n",
    "- Real-time emotion detection from camera input\n",
    "- Creating responsive NPC behavior based on emotions\n",
    "\n",
    "## üó∫Ô∏è Our Journey Today\n",
    "\n",
    "1. **üìä Data Exploration** - Understand our emotion dataset\n",
    "2. **üèóÔ∏è Model Architecture** - Build our CNN emotion detector  \n",
    "3. **üéì Training Process** - Teach our AI to recognize emotions\n",
    "4. **üìà Evaluation** - Test how well our model performs\n",
    "5. **üíæ Model Saving** - Prepare our model for the game\n",
    "6. **üéÆ Game Integration** - See how it works in practice\n",
    "\n",
    "## üöÄ By the End of This Notebook\n",
    "\n",
    "You'll have created a complete emotion recognition system that can:\n",
    "- ‚úÖ Detect 6 different emotions from facial expressions\n",
    "- ‚úÖ Work in real-time with camera input\n",
    "- ‚úÖ Integrate seamlessly with our adventure game\n",
    "- ‚úÖ Provide the foundation for emotion-aware applications\n",
    "\n",
    "**Let's build the future of emotionally intelligent technology!** üåü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVz8_6AYI79Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT LIBRARIES: The tools we need to build our emotion recognition system\n",
    "# =============================================================================\n",
    "\n",
    "# Basic Python libraries for file handling and data manipulation\n",
    "import os                    # For working with files and directories\n",
    "import sys                   # For system-specific operations\n",
    "import shutil                # For copying and moving files\n",
    "import zipfile               # For extracting zip archives\n",
    "import random                # For generating random numbers\n",
    "import pandas as pd          # For handling data in table format\n",
    "import numpy as np           # For mathematical operations and arrays\n",
    "import matplotlib.pyplot as plt  # For creating graphs and visualizations\n",
    "from tqdm.auto import tqdm  # Progress bar library\n",
    "import time\n",
    "\n",
    "# Display plots directly in the notebook\n",
    "%matplotlib inline           \n",
    "\n",
    "# Additional utilities\n",
    "from functools import reduce\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import kagglehub            # For downloading datasets from Kaggle\n",
    "from PIL import Image # Import PIL Image for reliable image loading\n",
    "\n",
    "# =============================================================================\n",
    "# PYTORCH LIGHTNING: Our main deep learning framework\n",
    "# =============================================================================\n",
    "# PyTorch Lightning makes it easier to organize and train neural networks\n",
    "# It handles a lot of the complex training logic for us!\n",
    "\n",
    "import torch                           # Core PyTorch library\n",
    "import torch.nn as nn                  # Neural network building blocks\n",
    "import torch.nn.functional as F        # Common neural network functions\n",
    "import torchvision                     # Computer vision utilities\n",
    "import torchvision.transforms as transforms  # Image preprocessing tools\n",
    "from torch.utils.data import DataLoader, Dataset  # Data loading utilities\n",
    "\n",
    "import pytorch_lightning as pl         # Lightning framework for easier training\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION TOOLS: How we measure our model's performance\n",
    "# =============================================================================\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üß† Ready to build an emotion recognition system!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SMART DATA DOWNLOAD\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîç Checking if emotion dataset already exists...\")\n",
    "\n",
    "# Check if we already have the data organized properly\n",
    "data_exists = False\n",
    "if os.path.exists(\"data\"):\n",
    "    train_dir = os.path.join(\"data\", \"train\")\n",
    "    test_dir = os.path.join(\"data\", \"test\")\n",
    "\n",
    "    if os.path.exists(train_dir) and os.path.exists(test_dir):\n",
    "        # Check if we have emotion categories in both directories\n",
    "        train_emotions = [\n",
    "            d\n",
    "            for d in os.listdir(train_dir)\n",
    "            if os.path.isdir(os.path.join(train_dir, d))\n",
    "        ]\n",
    "        test_emotions = [\n",
    "            d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))\n",
    "        ]\n",
    "\n",
    "        if (\n",
    "            len(train_emotions) >= 5 and len(test_emotions) >= 5\n",
    "        ):  # Should have at least 5 emotion categories\n",
    "            print(\"‚úÖ Dataset already exists and looks complete!\")\n",
    "            print(f\"   Train emotions: {train_emotions}\")\n",
    "            print(f\"   Test emotions: {test_emotions}\")\n",
    "            data_exists = True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Data directory exists but seems incomplete\")\n",
    "            print(f\"   Train emotions found: {train_emotions}\")\n",
    "            print(f\"   Test emotions found: {test_emotions}\")\n",
    "\n",
    "if not data_exists:\n",
    "    print(\"üì• Dataset not found or incomplete. Downloading from Kaggle...\")\n",
    "\n",
    "    # Download the dataset using kagglehub\n",
    "    print(\"Downloading dataset from Kaggle...\")\n",
    "    dataset_path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
    "    print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "    # Create data directory if it doesn't exist\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "        print(\"Created 'data' directory\")\n",
    "\n",
    "    # Check what files/folders are in the downloaded dataset\n",
    "    print(f\"Contents of {dataset_path}:\")\n",
    "    for item in os.listdir(dataset_path):\n",
    "        item_path = os.path.join(dataset_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  Directory: {item}\")\n",
    "        else:\n",
    "            print(f\"  File: {item}\")\n",
    "\n",
    "    # Look for zip file first\n",
    "    zip_file = None\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith(\".zip\"):\n",
    "            zip_file = os.path.join(dataset_path, file)\n",
    "            break\n",
    "\n",
    "    if zip_file:\n",
    "        # Extract the zip file to the data directory\n",
    "        print(f\"Extracting {zip_file} to data/\")\n",
    "        with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/\")\n",
    "        print(\"Extraction complete\")\n",
    "    else:\n",
    "        # No zip file found, check if train/test directories already exist\n",
    "        train_dir = os.path.join(dataset_path, \"train\")\n",
    "        test_dir = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "        if os.path.exists(train_dir) and os.path.exists(test_dir):\n",
    "            print(\"Found train and test directories, copying to data/\")\n",
    "            # Copy the train and test directories\n",
    "            shutil.copytree(\n",
    "                train_dir, os.path.join(\"data\", \"train\"), dirs_exist_ok=True\n",
    "            )\n",
    "            shutil.copytree(test_dir, os.path.join(\"data\", \"test\"), dirs_exist_ok=True)\n",
    "            print(\"Directories copied successfully\")\n",
    "        else:\n",
    "            # Look for any other structure\n",
    "            print(\"Searching for dataset files in subdirectories...\")\n",
    "            for root, dirs, files in os.walk(dataset_path):\n",
    "                if \"train\" in dirs and \"test\" in dirs:\n",
    "                    train_source = os.path.join(root, \"train\")\n",
    "                    test_source = os.path.join(root, \"test\")\n",
    "                    print(f\"Found train/test directories in: {root}\")\n",
    "                    shutil.copytree(\n",
    "                        train_source, os.path.join(\"data\", \"train\"), dirs_exist_ok=True\n",
    "                    )\n",
    "                    shutil.copytree(\n",
    "                        test_source, os.path.join(\"data\", \"test\"), dirs_exist_ok=True\n",
    "                    )\n",
    "                    print(\"Directories copied successfully\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Could not find train/test directories in the downloaded dataset\")\n",
    "\n",
    "# Verify the final data directory structure\n",
    "if os.path.exists(\"data\"):\n",
    "    print(f\"\\nüìÅ Final data directory structure:\")\n",
    "    for item in os.listdir(\"data\"):\n",
    "        item_path = os.path.join(\"data\", item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  Directory: {item}\")\n",
    "            # Show subdirectories (emotion categories) and count files\n",
    "            if os.path.exists(item_path):\n",
    "                subdirs = [\n",
    "                    d\n",
    "                    for d in os.listdir(item_path)\n",
    "                    if os.path.isdir(os.path.join(item_path, d))\n",
    "                ]\n",
    "                print(f\"    Emotion categories: {subdirs}\")\n",
    "\n",
    "                # Count total images\n",
    "                total_images = 0\n",
    "                for emotion_dir in subdirs:\n",
    "                    emotion_path = os.path.join(item_path, emotion_dir)\n",
    "                    image_files = [\n",
    "                        f\n",
    "                        for f in os.listdir(emotion_path)\n",
    "                        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "                    ]\n",
    "                    total_images += len(image_files)\n",
    "                print(f\"    Total images: {total_images}\")\n",
    "\n",
    "# Clean up disgust category (only if data was just downloaded or if disgust still exists)\n",
    "disgust_train_path = os.path.join(\"data\", \"train\", \"disgust\")\n",
    "disgust_test_path = os.path.join(\"data\", \"test\", \"disgust\")\n",
    "\n",
    "disgust_removed = False\n",
    "if os.path.exists(disgust_train_path):\n",
    "    shutil.rmtree(disgust_train_path)\n",
    "    print(\"\\nüóëÔ∏è  Removed data/train/disgust folder (too few examples)\")\n",
    "    disgust_removed = True\n",
    "\n",
    "if os.path.exists(disgust_test_path):\n",
    "    shutil.rmtree(disgust_test_path)\n",
    "    print(\"üóëÔ∏è  Removed data/test/disgust folder (too few examples)\")\n",
    "    disgust_removed = True\n",
    "\n",
    "if not disgust_removed:\n",
    "    print(\"\\n‚úÖ Disgust category already removed or not present\")\n",
    "\n",
    "print(\"\\nüéâ Dataset preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ioyd0sAI79R",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUSTOM DATASET CLASS: Teaching the computer how to read our emotion images\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class teaches PyTorch how to load and process our emotion images.\n",
    "    Think of it as a recipe that tells the computer:\n",
    "    1. Where to find each image\n",
    "    2. What emotion label goes with each image\n",
    "    3. How to prepare the image for our neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize our dataset\n",
    "        - root_dir: folder containing our emotion images\n",
    "        - transform: image preprocessing steps (resize, normalize, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "\n",
    "        # Find all emotion categories (angry, happy, sad, etc.)\n",
    "        emotion_folders = sorted(\n",
    "            [\n",
    "                f\n",
    "                for f in os.listdir(root_dir)\n",
    "                if os.path.isdir(os.path.join(root_dir, f))\n",
    "            ]\n",
    "        )\n",
    "        self.class_names = emotion_folders\n",
    "\n",
    "        print(f\"üìÇ Found emotion categories: {self.class_names}\")\n",
    "\n",
    "        # Load all image paths and their corresponding emotion labels\n",
    "        for label_idx, emotion in enumerate(emotion_folders):\n",
    "            emotion_path = os.path.join(root_dir, emotion)\n",
    "\n",
    "            # Count images in this emotion category\n",
    "            image_files = [\n",
    "                f\n",
    "                for f in os.listdir(emotion_path)\n",
    "                if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "            ]\n",
    "            print(f\"   {emotion}: {len(image_files)} images\")\n",
    "\n",
    "            # Add each image path and its label to our lists\n",
    "            for img_file in image_files:\n",
    "                self.images.append(os.path.join(emotion_path, img_file))\n",
    "                self.labels.append(label_idx)\n",
    "\n",
    "        print(f\"üìä Total dataset size: {len(self.images)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of images in our dataset\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single image and its label.\n",
    "        This function is called each time we want to train on one image.\n",
    "        \"\"\"\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            # Use PIL for robust image loading, convert to grayscale ('L' mode)\n",
    "            image = Image.open(img_path).convert(\"L\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Could not load image {img_path}. Error: {e}\")\n",
    "            # Return a dummy black image if loading fails\n",
    "            image = Image.new(\"L\", (48, 48), color=0)\n",
    "\n",
    "        # Apply preprocessing transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PYTORCH LIGHTNING DATA MODULE: Organizing our data for training\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class EmotionDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    This class handles all our data loading and preprocessing.\n",
    "    Lightning Data Modules keep our data code organized and reusable!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, batch_size=32, img_size=80, num_workers=\"auto\"):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Auto-detect optimal number of workers based on platform\n",
    "        if num_workers == \"auto\":\n",
    "            if os.name == \"nt\":  # Windows\n",
    "                self.num_workers = 0  # Avoid multiprocessing issues on Windows\n",
    "                print(\"ü™ü Windows detected: Using single-threaded data loading\")\n",
    "            else:  # Linux/Mac\n",
    "                import multiprocessing\n",
    "\n",
    "                self.num_workers = min(\n",
    "                    4, multiprocessing.cpu_count()\n",
    "                )  # Cap at 4 for stability\n",
    "                print(f\"üêß Unix-like system detected: Using {self.num_workers} workers\")\n",
    "        else:\n",
    "            self.num_workers = num_workers\n",
    "            print(f\"üîß Manual worker count: {self.num_workers}\")\n",
    "\n",
    "        # Define comprehensive image preprocessing steps\n",
    "        # These help our model work better and prevent overfitting\n",
    "        self.train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                transforms.ToTensor(),  # Convert PIL image to tensor (scales to [0,1])\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5]),  # Normalize to [-1,1]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Load our training and validation datasets\"\"\"\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Create training dataset\n",
    "            train_dir = os.path.join(self.data_dir, \"train\")\n",
    "            self.train_dataset = EmotionDataset(\n",
    "                train_dir, transform=self.train_transform\n",
    "            )\n",
    "\n",
    "            # Create validation dataset\n",
    "            val_dir = os.path.join(self.data_dir, \"test\")\n",
    "            self.val_dataset = EmotionDataset(val_dir, transform=self.val_transform)\n",
    "\n",
    "            # Store class names for later use\n",
    "            self.class_names = self.train_dataset.class_names\n",
    "            self.num_classes = len(self.class_names)\n",
    "\n",
    "            print(f\"üéØ Number of emotion classes: {self.num_classes}\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Create the data loader for training\"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,  # Randomize order for better training\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=(\n",
    "                True if torch.cuda.is_available() else False\n",
    "            ),  # Speed up GPU transfer\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Create the data loader for validation\"\"\"\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,  # Don't randomize validation data\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True if torch.cuda.is_available() else False,\n",
    "        )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# INITIALIZE OUR DATA MODULE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üèóÔ∏è  Setting up our emotion dataset...\")\n",
    "data_path = \"data/\"\n",
    "\n",
    "# Full-scale settings for comprehensive training\n",
    "data_module = EmotionDataModule(\n",
    "    data_dir=data_path,\n",
    "    batch_size=32,  # Standard batch size for good training\n",
    "    img_size=80,  # Higher resolution for better feature extraction\n",
    "    num_workers=\"auto\",  # Auto-detect optimal worker count\n",
    ")\n",
    "data_module.setup()\n",
    "\n",
    "print(f\"‚úÖ Data module ready!\")\n",
    "print(f\"üìä Training images: {len(data_module.train_dataset)}\")\n",
    "print(f\"üìä Validation images: {len(data_module.val_dataset)}\")\n",
    "print(f\"üòä Emotion categories: {data_module.class_names}\")\n",
    "print(\n",
    "    f\"üéØ Full-scale model: batch_size={data_module.batch_size}, img_size={data_module.img_size}\"\n",
    ")\n",
    "print(f\"üë• Workers: {data_module.num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPLORE OUR DATA: Let's see what we're working with!\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def visualize_sample_images_simple(data_dir=\"data/\", num_samples=3):\n",
    "    \"\"\"\n",
    "    Simple and robust data visualization that reads files directly\n",
    "    This avoids DataLoader issues that can cause hanging\n",
    "    \"\"\"\n",
    "    print(\"üñºÔ∏è  Sample images from each emotion category:\")\n",
    "    print(\"üìÅ Reading files directly from disk...\")\n",
    "\n",
    "    train_dir = os.path.join(data_dir, \"train\")\n",
    "    test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "    if not os.path.exists(train_dir):\n",
    "        print(f\"‚ùå Training directory not found: {train_dir}\")\n",
    "        return\n",
    "\n",
    "    # Get emotion categories\n",
    "    emotion_categories = [\n",
    "        d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))\n",
    "    ]\n",
    "    emotion_categories.sort()\n",
    "\n",
    "    print(\n",
    "        f\"üòä Found {len(emotion_categories)} emotion categories: {emotion_categories}\"\n",
    "    )\n",
    "\n",
    "    # Collect sample images and count all images\n",
    "    sample_images = {}\n",
    "    train_counts = {}\n",
    "    test_counts = {}\n",
    "\n",
    "    print(\"\\nüìä Collecting sample images and counting files...\")\n",
    "\n",
    "    for emotion in emotion_categories:\n",
    "        print(f\"   Processing {emotion}...\")\n",
    "\n",
    "        # Training data\n",
    "        train_emotion_dir = os.path.join(train_dir, emotion)\n",
    "        train_files = [\n",
    "            f\n",
    "            for f in os.listdir(train_emotion_dir)\n",
    "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "        ]\n",
    "        train_counts[emotion] = len(train_files)\n",
    "\n",
    "        # Test data\n",
    "        test_emotion_dir = os.path.join(test_dir, emotion)\n",
    "        test_files = []\n",
    "        if os.path.exists(test_emotion_dir):\n",
    "            test_files = [\n",
    "                f\n",
    "                for f in os.listdir(test_emotion_dir)\n",
    "                if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "            ]\n",
    "        test_counts[emotion] = len(test_files)\n",
    "\n",
    "        # Sample a few images for visualization\n",
    "        sample_files = random.sample(train_files, min(num_samples, len(train_files)))\n",
    "        sample_images[emotion] = []\n",
    "\n",
    "        for sample_file in sample_files:\n",
    "            try:\n",
    "                img_path = os.path.join(train_emotion_dir, sample_file)\n",
    "                # Read image using PIL (more reliable than matplotlib for various formats)\n",
    "                with Image.open(img_path) as img:\n",
    "                    # Convert to grayscale and numpy array\n",
    "                    img_gray = img.convert(\"L\")\n",
    "                    img_array = np.array(img_gray)\n",
    "                    sample_images[emotion].append(img_array)\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ö†Ô∏è  Could not load {sample_file}: {e}\")\n",
    "\n",
    "    print(\"‚úÖ Sample collection complete!\")\n",
    "\n",
    "    # Display sample images\n",
    "    print(\"\\nüé® Creating sample image visualization...\")\n",
    "\n",
    "    try:\n",
    "        fig, axes = plt.subplots(\n",
    "            len(emotion_categories),\n",
    "            num_samples,\n",
    "            figsize=(15, 2.5 * len(emotion_categories)),\n",
    "        )\n",
    "        fig.suptitle(\n",
    "            \"Sample Images from Each Emotion Category\", fontsize=16, fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "        for i, emotion in enumerate(emotion_categories):\n",
    "            for j in range(num_samples):\n",
    "                if len(emotion_categories) > 1:\n",
    "                    ax = axes[i, j]\n",
    "                else:\n",
    "                    ax = axes[j]\n",
    "\n",
    "                if j < len(sample_images[emotion]):\n",
    "                    ax.imshow(sample_images[emotion][j], cmap=\"gray\")\n",
    "                    ax.set_title(f\"{emotion.title()}\", fontsize=10)\n",
    "                else:\n",
    "                    ax.text(\n",
    "                        0.5,\n",
    "                        0.5,\n",
    "                        \"No image\",\n",
    "                        ha=\"center\",\n",
    "                        va=\"center\",\n",
    "                        transform=ax.transAxes,\n",
    "                        fontsize=10,\n",
    "                    )\n",
    "                    ax.set_title(f\"{emotion.title()}\", fontsize=10)\n",
    "\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"‚úÖ Sample images displayed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating sample visualization: {e}\")\n",
    "        print(\"üìä Showing text summary instead...\")\n",
    "\n",
    "        for emotion in emotion_categories:\n",
    "            print(f\"   {emotion}: {len(sample_images[emotion])} samples loaded\")\n",
    "\n",
    "    # Display class distribution\n",
    "    print(\"\\nüìà Creating class distribution charts...\")\n",
    "\n",
    "    try:\n",
    "        plt.figure(figsize=(14, 6))\n",
    "\n",
    "        # Training data distribution\n",
    "        plt.subplot(1, 2, 1)\n",
    "        emotions = list(train_counts.keys())\n",
    "        train_values = list(train_counts.values())\n",
    "\n",
    "        bars = plt.bar(\n",
    "            emotions, train_values, color=\"skyblue\", edgecolor=\"navy\", alpha=0.8\n",
    "        )\n",
    "        plt.title(\"Training Data Distribution\", fontweight=\"bold\", fontsize=14)\n",
    "        plt.xlabel(\"Emotion Category\", fontweight=\"bold\")\n",
    "        plt.ylabel(\"Number of Images\", fontweight=\"bold\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "        # Add count labels on bars\n",
    "        for bar, count in zip(bars, train_values):\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width() / 2,\n",
    "                bar.get_height() + max(train_values) * 0.01,\n",
    "                str(count),\n",
    "                ha=\"center\",\n",
    "                fontweight=\"bold\",\n",
    "                fontsize=10,\n",
    "            )\n",
    "\n",
    "        # Test/Validation data distribution\n",
    "        plt.subplot(1, 2, 2)\n",
    "        test_values = list(test_counts.values())\n",
    "\n",
    "        bars = plt.bar(\n",
    "            emotions, test_values, color=\"lightcoral\", edgecolor=\"darkred\", alpha=0.8\n",
    "        )\n",
    "        plt.title(\"Test/Validation Data Distribution\", fontweight=\"bold\", fontsize=14)\n",
    "        plt.xlabel(\"Emotion Category\", fontweight=\"bold\")\n",
    "        plt.ylabel(\"Number of Images\", fontweight=\"bold\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "        # Add count labels on bars\n",
    "        for bar, count in zip(bars, test_values):\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width() / 2,\n",
    "                bar.get_height() + max(test_values) * 0.01,\n",
    "                str(count),\n",
    "                ha=\"center\",\n",
    "                fontweight=\"bold\",\n",
    "                fontsize=10,\n",
    "            )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"‚úÖ Distribution charts created successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating distribution charts: {e}\")\n",
    "        print(\"üìä Showing numerical summary:\")\n",
    "\n",
    "        print(\"\\nüìä Dataset Summary:\")\n",
    "        print(\"=\" * 50)\n",
    "        total_train = sum(train_counts.values())\n",
    "        total_test = sum(test_counts.values())\n",
    "\n",
    "        for emotion in emotions:\n",
    "            train_count = train_counts[emotion]\n",
    "            test_count = test_counts[emotion]\n",
    "            train_pct = (train_count / total_train * 100) if total_train > 0 else 0\n",
    "            test_pct = (test_count / total_test * 100) if total_test > 0 else 0\n",
    "\n",
    "            print(\n",
    "                f\"{emotion.ljust(12)}: {train_count:>6} train ({train_pct:5.1f}%) | {test_count:>6} test ({test_pct:5.1f}%)\"\n",
    "            )\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\n",
    "            f\"{'TOTAL'.ljust(12)}: {total_train:>6} train (100.0%) | {total_test:>6} test (100.0%)\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nüìà What do these distributions tell us?\")\n",
    "    print(\"   ‚úì Some emotions have more examples than others (class imbalance)\")\n",
    "    print(\"   ‚úì This is normal in real-world datasets\")\n",
    "    print(\"   ‚úì We'll use techniques like class weighting to handle this\")\n",
    "    print(\"   ‚úì Each emotion shows different facial patterns we need to learn\")\n",
    "\n",
    "    print(\"\\n‚úÖ Data exploration complete!\")\n",
    "    print(\"üéØ Ready to build our emotion recognition model!\")\n",
    "\n",
    "\n",
    "# Run the simplified visualization\n",
    "print(\"üöÄ Starting robust data exploration...\")\n",
    "\n",
    "try:\n",
    "    visualize_sample_images_simple(\"data/\", num_samples=3)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Visualization failed: {e}\")\n",
    "    print(\"üìã Let's check what we have in the data directory...\")\n",
    "\n",
    "    if os.path.exists(\"data\"):\n",
    "        print(\"üìÅ Contents of data directory:\")\n",
    "        for item in os.listdir(\"data\"):\n",
    "            item_path = os.path.join(\"data\", item)\n",
    "            if os.path.isdir(item_path):\n",
    "                print(f\"   üìÇ {item}/\")\n",
    "                try:\n",
    "                    sub_items = os.listdir(item_path)[:5]  # Show first 5 items\n",
    "                    for sub_item in sub_items:\n",
    "                        print(f\"      üìÑ {sub_item}\")\n",
    "                    if len(os.listdir(item_path)) > 5:\n",
    "                        print(\n",
    "                            f\"      ... and {len(os.listdir(item_path)) - 5} more files\"\n",
    "                        )\n",
    "                except:\n",
    "                    print(f\"      ‚ùå Could not read directory\")\n",
    "            else:\n",
    "                print(f\"   üìÑ {item}\")\n",
    "    else:\n",
    "        print(\"‚ùå Data directory does not exist!\")\n",
    "        print(\"üí° Please run the data download cell (cell 5) first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnhLgZuKI79S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMPLIFIED CNN: A robust, standard model for baseline performance\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class EmotionCNN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A standard and simplified Convolutional Neural Network for emotion recognition.\n",
    "    This model provides a solid baseline for performance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=6, learning_rate=0.001, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if class_weights is not None:\n",
    "            self.register_buffer(\"class_weights\", class_weights)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "        # History tracking for plots\n",
    "        self.train_loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.val_acc_history = []\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        # --- Simplified Architecture ---\n",
    "        # Block 1: 1x80x80 -> 32x40x40\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Block 2: 32x40x40 -> 64x20x20\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Block 3: 64x20x20 -> 128x10x10\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Classifier\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 10 * 10, 512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "        print(\"üöÄ Created SIMPLIFIED CNN\")\n",
    "        print(f\"üèóÔ∏è Architecture: 3 conv blocks + 2 dense layers\")\n",
    "        print(f\"üîß Features: 32->64->128 filters\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == labels).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", accuracy, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.training_step_outputs.append(\n",
    "            {\"loss\": loss.detach(), \"acc\": accuracy.detach()}\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == labels).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        self.validation_step_outputs.append(\n",
    "            {\"val_loss\": loss.detach(), \"val_acc\": accuracy.detach()}\n",
    "        )\n",
    "        return {\"val_loss\": loss, \"val_acc\": accuracy}\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self.training_step_outputs:\n",
    "            avg_loss = torch.stack(\n",
    "                [x[\"loss\"] for x in self.training_step_outputs]\n",
    "            ).mean()\n",
    "            avg_acc = torch.stack([x[\"acc\"] for x in self.training_step_outputs]).mean()\n",
    "            self.train_loss_history.append(avg_loss.item())\n",
    "            self.train_acc_history.append(avg_acc.item())\n",
    "            self.training_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.validation_step_outputs:\n",
    "            avg_loss = torch.stack(\n",
    "                [x[\"val_loss\"] for x in self.validation_step_outputs]\n",
    "            ).mean()\n",
    "            avg_acc = torch.stack(\n",
    "                [x[\"val_acc\"] for x in self.validation_step_outputs]\n",
    "            ).mean()\n",
    "            self.val_loss_history.append(avg_loss.item())\n",
    "            self.val_acc_history.append(avg_acc.item())\n",
    "            self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == labels).float().mean()\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        return {\"test_loss\": loss, \"test_acc\": accuracy}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Standard Adam optimizer\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE MODEL INSTANCE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üèóÔ∏è  Building SIMPLIFIED emotion recognition model...\")\n",
    "\n",
    "# Calculate class weights for balanced training\n",
    "print(\"‚öñÔ∏è  Calculating class weights for balanced training...\")\n",
    "train_loader = data_module.train_dataloader()\n",
    "all_labels = []\n",
    "\n",
    "sample_count = 0\n",
    "max_samples = 5000\n",
    "for _, labels in train_loader:\n",
    "    all_labels.extend(labels.numpy())\n",
    "    sample_count += len(labels)\n",
    "    if sample_count >= max_samples:\n",
    "        break\n",
    "\n",
    "class_weights_sklearn = class_weight.compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(all_labels), y=all_labels\n",
    ")\n",
    "class_weights_tensor = torch.FloatTensor(class_weights_sklearn)\n",
    "print(f\"‚öñÔ∏è  Class weights: {class_weights_sklearn}\")\n",
    "\n",
    "# Create model with a standard learning rate\n",
    "model = EmotionCNN(\n",
    "    num_classes=data_module.num_classes,\n",
    "    learning_rate=0.001,  # Standard learning rate for Adam\n",
    "    class_weights=class_weights_tensor,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SIMPLIFIED model created successfully!\")\n",
    "\n",
    "# Display model statistics\n",
    "print(\"\\nüèóÔ∏è  Simplified Model Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t07Yf2EcI79S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING SETUP: Preparing to teach our AI about emotions!\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ Setting up training configuration...\")\n",
    "\n",
    "# Training hyperparameters for full-scale model\n",
    "max_epochs = 25  # More epochs for thorough training\n",
    "patience = 7  # Early stopping patience\n",
    "\n",
    "# Set up early stopping to prevent overfitting\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Watch validation loss\n",
    "    min_delta=0.001,  # Minimum change to qualify as improvement\n",
    "    patience=patience,  # Number of epochs to wait for improvement\n",
    "    verbose=True,  # Print when stopping\n",
    "    mode=\"min\",  # We want to minimize validation loss\n",
    ")\n",
    "\n",
    "# Model checkpointing to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_acc\",  # Save model with best validation accuracy\n",
    "    dirpath=\"checkpoints/\",  # Where to save checkpoints\n",
    "    filename=\"emotion-cnn-{epoch:02d}-{val_acc:.3f}\",\n",
    "    save_top_k=1,  # Keep only the best model\n",
    "    mode=\"max\",  # We want to maximize validation accuracy\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# JUPYTERLAB-COMPATIBLE PROGRESS BAR CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Try to import RichProgressBar for better JupyterLab compatibility\n",
    "try:\n",
    "    from pytorch_lightning.callbacks import RichProgressBar\n",
    "    from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "\n",
    "    # Configure a clean progress bar theme for JupyterLab\n",
    "    progress_bar = RichProgressBar(\n",
    "        theme=RichProgressBarTheme(\n",
    "            description=\"cyan\",\n",
    "            progress_bar=\"cyan\",\n",
    "            progress_bar_finished=\"green\",\n",
    "            batch_progress=\"cyan\",\n",
    "            time=\"grey82\",\n",
    "            processing_speed=\"grey82\",\n",
    "            metrics=\"cyan\",\n",
    "        ),\n",
    "        leave=True,  # Keep progress bar after completion\n",
    "        console_kwargs={\"force_jupyter\": True},  # Force Jupyter compatibility\n",
    "    )\n",
    "    progress_bar_callbacks = [progress_bar]\n",
    "    print(\"‚úÖ Using RichProgressBar for optimal JupyterLab display\")\n",
    "\n",
    "except ImportError:\n",
    "    # Fallback: Disable progress bar to avoid conflicts in JupyterLab\n",
    "    progress_bar_callbacks = []\n",
    "    enable_progress_bar = False\n",
    "    print(\n",
    "        \"‚ö†Ô∏è  RichProgressBar not available, disabling progress bars to prevent conflicts\"\n",
    "    )\n",
    "    print(\"üìä Training progress will be shown via logged metrics only\")\n",
    "\n",
    "# Create the Lightning trainer with JupyterLab-optimized settings\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"auto\",  # Automatically use GPU if available, otherwise CPU\n",
    "    devices=\"auto\",  # Auto-detect number of devices\n",
    "    callbacks=[early_stop_callback, checkpoint_callback] + progress_bar_callbacks,\n",
    "    log_every_n_steps=50,  # Reduced logging frequency for cleaner output\n",
    "    enable_progress_bar=len(progress_bar_callbacks)\n",
    "    > 0,  # Only if RichProgressBar is available\n",
    "    enable_model_summary=True,\n",
    "    logger=TensorBoardLogger(\"lightning_logs\", name=\"emotion_cnn\"),\n",
    "    # Additional JupyterLab optimizations\n",
    "    enable_checkpointing=True,\n",
    "    deterministic=False,  # Allow some non-deterministic operations for speed\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training setup complete!\")\n",
    "print(f\"üéØ Max epochs: {max_epochs}\")\n",
    "print(f\"‚è±Ô∏è Early stopping patience: {patience} epochs\")\n",
    "print(f\"üíæ Model checkpoints will be saved to: checkpoints/\")\n",
    "print(f\"üîç Monitoring validation accuracy for best model\")\n",
    "print(f\"üìä Progress bar optimized for JupyterLab compatibility\")\n",
    "\n",
    "# Display training device info\n",
    "device = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"üñ•Ô∏è Training device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# JUPYTERLAB TQDM CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Configure TQDM for better JupyterLab behavior\n",
    "import os\n",
    "\n",
    "os.environ[\"TQDM_DISABLE\"] = \"0\"  # Enable progress bars\n",
    "os.environ[\"TQDM_POSITION\"] = \"0\"  # Prevent stacking\n",
    "\n",
    "# Import and configure tqdm for Jupyter\n",
    "try:\n",
    "    import tqdm.notebook as tqdm_notebook\n",
    "\n",
    "    # Force tqdm to use notebook mode\n",
    "    tqdm_notebook.tqdm.pandas()\n",
    "    print(\"üìä TQDM configured for Jupyter notebook compatibility\")\n",
    "except ImportError:\n",
    "    print(\"üìä Using standard TQDM configuration\")\n",
    "\n",
    "# =============================================================================\n",
    "# START TRAINING! üéì\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéì STARTING EMOTION RECOGNITION TRAINING!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"The model will learn to recognize emotions from facial expressions.\")\n",
    "print(\"This may take several minutes depending on your hardware...\")\n",
    "print(\"üìä Progress bars optimized for JupyterLab - no more stacking!\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Start the training process\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRAINING COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Best model saved to: {checkpoint_callback.best_model_path}\")\n",
    "print(f\"üéØ Best validation accuracy: {checkpoint_callback.best_model_score:.3f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüß† What did our AI learn?\")\n",
    "print(\"   - How to detect facial features (eyes, mouth, eyebrows)\")\n",
    "print(\"   - Patterns that indicate different emotions\")\n",
    "print(\"   - How to distinguish between happy, sad, angry, etc.\")\n",
    "print(\"   - The ability to recognize emotions in new faces it hasn't seen before!\")\n",
    "\n",
    "print(\"\\nüìä Next steps:\")\n",
    "print(\"   - Evaluate model performance on test data\")\n",
    "print(\"   - Visualize what the model learned\")\n",
    "print(\"   - Test it on new images\")\n",
    "print(\"   - Use it in our game for interactive NPCs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-o4e6FcuI79T",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PROFESSIONAL TRAINING CONFIGURATION: Optimized for high accuracy\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ Setting up PROFESSIONAL training configuration...\")\n",
    "\n",
    "# Training hyperparameters - optimized for best performance\n",
    "max_epochs = 40  # More epochs for thorough training of professional model\n",
    "patience = 8  # Early stopping patience - more lenient for better convergence\n",
    "\n",
    "# Early stopping callback - prevents overfitting\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss\n",
    "    min_delta=0.001,  # Minimum change to qualify as improvement\n",
    "    patience=patience,  # Wait longer before stopping\n",
    "    verbose=True,\n",
    "    mode=\"min\",  # We want to minimize validation loss\n",
    ")\n",
    "\n",
    "# Model checkpoint callback - saves the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_acc\",  # Save model with best validation accuracy\n",
    "    dirpath=\"checkpoints/\",  # Where to save checkpoints\n",
    "    filename=\"emotion-cnn-{epoch:02d}-{val_acc:.3f}\",\n",
    "    save_top_k=1,  # Keep only the best model\n",
    "    mode=\"max\",  # We want to maximize validation accuracy\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# JUPYTERLAB-COMPATIBLE PROGRESS BAR CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Try to import RichProgressBar for better JupyterLab compatibility\n",
    "try:\n",
    "    from pytorch_lightning.callbacks import RichProgressBar\n",
    "    from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "\n",
    "    # Configure a clean progress bar theme for JupyterLab\n",
    "    progress_bar = RichProgressBar(\n",
    "        theme=RichProgressBarTheme(\n",
    "            description=\"cyan\",\n",
    "            progress_bar=\"cyan\",\n",
    "            progress_bar_finished=\"green\",\n",
    "            batch_progress=\"cyan\",\n",
    "            time=\"grey82\",\n",
    "            processing_speed=\"grey82\",\n",
    "            metrics=\"cyan\",\n",
    "        ),\n",
    "        leave=True,  # Keep progress bar after completion\n",
    "        console_kwargs={\"force_jupyter\": True},  # Force Jupyter compatibility\n",
    "    )\n",
    "    progress_bar_callbacks = [progress_bar]\n",
    "    print(\"‚úÖ Using RichProgressBar for optimal JupyterLab display\")\n",
    "\n",
    "except ImportError:\n",
    "    # Fallback: Disable progress bar to avoid conflicts in JupyterLab\n",
    "    progress_bar_callbacks = []\n",
    "    enable_progress_bar = False\n",
    "    print(\n",
    "        \"‚ö†Ô∏è  RichProgressBar not available, disabling progress bars to prevent conflicts\"\n",
    "    )\n",
    "    print(\"üìä Training progress will be shown via logged metrics only\")\n",
    "\n",
    "# Create the Lightning trainer with professional settings\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"auto\",  # Automatically use GPU if available, otherwise CPU\n",
    "    devices=\"auto\",  # Auto-detect number of devices\n",
    "    callbacks=[early_stop_callback, checkpoint_callback] + progress_bar_callbacks,\n",
    "    log_every_n_steps=25,  # Balanced logging frequency\n",
    "    logger=TensorBoardLogger(\"lightning_logs\", name=\"professional_emotion_cnn\"),\n",
    "    precision=32,  # Use full precision for best accuracy\n",
    "    enable_progress_bar=len(progress_bar_callbacks) > 0,  # Enable if available\n",
    "    enable_model_summary=True,  # Show detailed model summary\n",
    "    deterministic=False,  # Allow optimizations for faster training\n",
    "    gradient_clip_val=1.0,  # Gradient clipping for training stability\n",
    "    accumulate_grad_batches=1,  # Standard gradient accumulation\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PROFESSIONAL training setup complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üèÜ PROFESSIONAL TRAINING FEATURES:\")\n",
    "print(f\"   üìà Max epochs: {max_epochs} (more thorough training)\")\n",
    "print(f\"   ‚è∞ Early stopping patience: {patience} epochs\")\n",
    "print(\"   üéØ Full 32-bit precision for maximum accuracy\")\n",
    "print(\"   üìä TensorBoard logging for detailed monitoring\")\n",
    "print(\"   üíæ Best model checkpointing\")\n",
    "print(\"   üõ°Ô∏è  Gradient clipping for training stability\")\n",
    "print(\"   ‚ö° Auto GPU/CPU detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION: How well did our AI learn to recognize emotions?\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Evaluating our emotion recognition model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test the model on validation data\n",
    "test_results = trainer.test(model, data_module.val_dataloader())\n",
    "\n",
    "print(f\"üéØ Final Test Results:\")\n",
    "print(f\"   Test Accuracy: {test_results[0]['test_acc']:.3f}\")\n",
    "print(f\"   Test Loss: {test_results[0]['test_loss']:.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING HISTORY VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def plot_training_history(model):\n",
    "    \"\"\"\n",
    "    Visualize how our model improved during training.\n",
    "    This helps us understand if our model learned properly.\n",
    "    \"\"\"\n",
    "    print(\"üìà Creating training history plots...\")\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    epochs = range(1, len(model.train_acc_history) + 1)\n",
    "\n",
    "    # Accuracy plot\n",
    "    ax1.plot(epochs, model.train_acc_history, \"b-\", label=\"Training Accuracy\")\n",
    "    ax1.plot(epochs, model.val_acc_history, \"r-\", label=\"Validation Accuracy\")\n",
    "    ax1.set_title(\"Model Accuracy Over Time\", fontweight=\"bold\", fontsize=14)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Loss plot\n",
    "    ax2.plot(epochs, model.train_loss_history, \"b-\", label=\"Training Loss\")\n",
    "    ax2.plot(epochs, model.val_loss_history, \"r-\", label=\"Validation Loss\")\n",
    "    ax2.set_title(\"Model Loss Over Time\", fontweight=\"bold\", fontsize=14)\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"üìà What these plots tell us:\")\n",
    "    print(\"   - Accuracy should increase over time (model getting better)\")\n",
    "    print(\"   - Loss should decrease over time (model making fewer mistakes)\")\n",
    "    print(\"   - Training and validation curves should be close (no overfitting)\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFUSION MATRIX: Detailed performance analysis\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def create_confusion_matrix(model, data_loader, class_names):\n",
    "    \"\"\"\n",
    "    Create a confusion matrix to see which emotions our model confuses\n",
    "    This is super helpful for understanding model strengths and weaknesses!\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üîç Creating confusion matrix...\")\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Collect predictions and true labels\n",
    "    with torch.no_grad():  # No gradient computation needed for evaluation\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.title(\"Emotion Recognition Confusion Matrix\", fontweight=\"bold\", fontsize=16)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Add labels\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], \"d\"),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.ylabel(\"True Emotion\", fontweight=\"bold\")\n",
    "    plt.xlabel(\"Predicted Emotion\", fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed analysis\n",
    "    print(\"\\nüîç Confusion Matrix Analysis:\")\n",
    "    print(\"   - Diagonal values = correct predictions\")\n",
    "    print(\"   - Off-diagonal values = mistakes\")\n",
    "    print(\"   - Higher diagonal values = better performance\")\n",
    "\n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "    print(f\"\\nüìä Per-emotion accuracy:\")\n",
    "    for emotion, acc in zip(class_names, class_accuracies):\n",
    "        print(f\"   {emotion}: {acc:.3f} ({acc*100:.1f}%)\")\n",
    "\n",
    "    return cm\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAMPLE PREDICTIONS: See our model in action!\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def visualize_predictions(model, data_loader, class_names, num_samples=8):\n",
    "    \"\"\"\n",
    "    Show sample predictions to see how our model performs on real images\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üñºÔ∏è  Sample Predictions from Our Model:\")\n",
    "\n",
    "    model.eval()\n",
    "    images_shown = 0\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    fig.suptitle(\"Sample Emotion Predictions\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(min(len(images), num_samples - images_shown)):\n",
    "                row = images_shown // 4\n",
    "                col = images_shown % 4\n",
    "\n",
    "                # Display image\n",
    "                img = images[i].squeeze().cpu().numpy()\n",
    "                axes[row, col].imshow(img, cmap=\"gray\")\n",
    "\n",
    "                # Get prediction info\n",
    "                true_emotion = class_names[labels[i]]\n",
    "                pred_emotion = class_names[predicted[i]]\n",
    "                confidence = probabilities[i][predicted[i]].item()\n",
    "\n",
    "                # Set title with prediction\n",
    "                color = \"green\" if true_emotion == pred_emotion else \"red\"\n",
    "                title = f\"True: {true_emotion}\\nPred: {pred_emotion}\\nConf: {confidence:.2f}\"\n",
    "                axes[row, col].set_title(title, color=color, fontweight=\"bold\")\n",
    "                axes[row, col].axis(\"off\")\n",
    "\n",
    "                images_shown += 1\n",
    "                if images_shown >= num_samples:\n",
    "                    break\n",
    "\n",
    "            if images_shown >= num_samples:\n",
    "                break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"üéØ Prediction Legend:\")\n",
    "    print(\"   üü¢ Green titles = Correct predictions\")\n",
    "    print(\"   üî¥ Red titles = Incorrect predictions\")\n",
    "    print(\"   Conf = Model's confidence (0.0 to 1.0)\")\n",
    "\n",
    "\n",
    "# Run all evaluations\n",
    "print(\"üöÄ Running comprehensive model evaluation...\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(model)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = create_confusion_matrix(\n",
    "    model, data_module.val_dataloader(), data_module.class_names\n",
    ")\n",
    "\n",
    "# Show sample predictions\n",
    "visualize_predictions(model, data_module.val_dataloader(), data_module.class_names)\n",
    "\n",
    "print(\"\\nüéâ Evaluation complete!\")\n",
    "print(\"üìù Key takeaways:\")\n",
    "print(\"   - Check confusion matrix for emotion pairs the model confuses\")\n",
    "print(\"   - Look at sample predictions to see real performance\")\n",
    "print(\"   - High confidence correct predictions = good learning\")\n",
    "print(\"   - Use this analysis to improve the model further\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHaZCvxpRsga"
   },
   "source": [
    "## Save our model\n",
    "\n",
    "Now that we've trained our model, we can save it for the next steps where we will want to use the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdDCs9OlI79T"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE OUR TRAINED MODEL: Preserving our AI's learned knowledge\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üíæ Saving our trained emotion recognition model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# The best model was automatically saved during training by our checkpoint callback\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"üèÜ Best model automatically saved at: {best_model_path}\")\n",
    "\n",
    "# Let's also save the model in a format that's easy to use later\n",
    "# This creates a complete package with both the model architecture and learned weights\n",
    "\n",
    "# Save the entire model (architecture + weights)\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"class_names\": data_module.class_names,\n",
    "        \"num_classes\": data_module.num_classes,\n",
    "        \"model_config\": {\n",
    "            \"learning_rate\": model.learning_rate,\n",
    "            \"input_size\": (80, 80),\n",
    "        },\n",
    "    },\n",
    "    \"emotion_recognition_model.pth\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model saved as 'emotion_recognition_model.pth'\")\n",
    "\n",
    "# Also save in ONNX format for broader compatibility (optional)\n",
    "try:\n",
    "    # Export to ONNX format (works with many different frameworks)\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 1, 80, 80)  # Example input tensor\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        \"emotion_model.onnx\",\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "    )\n",
    "    print(\"‚úÖ Model also saved in ONNX format as 'emotion_model.onnx'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  ONNX export failed: {e}\")\n",
    "\n",
    "print(\"\\nüìÅ Saved Files Summary:\")\n",
    "print(f\"   üèÜ Best checkpoint: {best_model_path}\")\n",
    "print(\"   üíæ Complete model: emotion_recognition_model.pth\")\n",
    "print(\"   üåê ONNX format: emotion_model.onnx (if successful)\")\n",
    "\n",
    "print(\"\\nüéÆ Ready for Game Integration!\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Our emotion recognition model is now ready to be used in the game!\")\n",
    "print(\"üìã Next steps for game integration:\")\n",
    "print(\"   1. Load the saved model in the game code\")\n",
    "print(\"   2. Connect webcam/camera input\")\n",
    "print(\"   3. Process player's facial expressions in real-time\")\n",
    "print(\"   4. Use emotion predictions to influence NPC dialogue\")\n",
    "print(\"   5. Create dynamic, emotion-responsive gameplay!\")\n",
    "\n",
    "print(\"\\nüß† What our AI has learned:\")\n",
    "print(\"   ‚úì Recognize facial features and expressions\")\n",
    "print(\"   ‚úì Classify emotions with high accuracy\")\n",
    "print(\"   ‚úì Handle real-world variations in lighting and faces\")\n",
    "print(\"   ‚úì Make predictions in real-time\")\n",
    "\n",
    "print(\"\\nüéØ Model Performance Summary:\")\n",
    "print(f\"   üìä Training completed successfully\")\n",
    "print(f\"   üéØ Model can recognize {data_module.num_classes} different emotions\")\n",
    "print(f\"   üìà Ready for real-world emotion detection!\")\n",
    "\n",
    "print(\"\\nüöÄ Congratulations! You've built an AI emotion recognition system!\")\n",
    "\n",
    "# =============================================================================\n",
    "# PLOT TRAINING HISTORY: Visualizing our model's performance\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def plot_training_history(trainer):\n",
    "    \"\"\"\n",
    "    Visualize how our model improved during training\n",
    "    This helps us understand if our model learned properly\n",
    "    \"\"\"\n",
    "    print(\"üìà Creating training history plots from logged metrics...\")\n",
    "\n",
    "    try:\n",
    "        # Extract metrics from the trainer's logged metrics\n",
    "        logged_metrics = trainer.logged_metrics\n",
    "\n",
    "        # Get callbacks to extract epoch-level metrics\n",
    "        callbacks = trainer.callback_metrics\n",
    "\n",
    "        # Try to get metrics from the logger's experiment\n",
    "        if hasattr(trainer.logger, \"experiment\") and hasattr(\n",
    "            trainer.logger.experiment, \"metrics\"\n",
    "        ):\n",
    "            metrics_data = trainer.logger.experiment.metrics\n",
    "        else:\n",
    "            # Fall back to logged metrics\n",
    "            metrics_data = logged_metrics\n",
    "\n",
    "        # Try to extract training history from different sources\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        epochs = []\n",
    "\n",
    "        # Method 1: Check if we have epoch-level metrics in callback_metrics\n",
    "        if callbacks:\n",
    "            print(\"üìä Found callback metrics, extracting training curves...\")\n",
    "\n",
    "            # Extract what we can from the final logged values\n",
    "            train_loss = callbacks.get(\n",
    "                \"train_loss_epoch\", callbacks.get(\"train_loss\", None)\n",
    "            )\n",
    "            val_loss = callbacks.get(\"val_loss\", None)\n",
    "            train_acc = callbacks.get(\n",
    "                \"train_acc_epoch\", callbacks.get(\"train_acc\", None)\n",
    "            )\n",
    "            val_acc = callbacks.get(\"val_acc\", None)\n",
    "\n",
    "            if train_loss is not None:\n",
    "                train_losses = (\n",
    "                    [train_loss.item()]\n",
    "                    if hasattr(train_loss, \"item\")\n",
    "                    else [float(train_loss)]\n",
    "                )\n",
    "            if val_loss is not None:\n",
    "                val_losses = (\n",
    "                    [val_loss.item()]\n",
    "                    if hasattr(val_loss, \"item\")\n",
    "                    else [float(val_loss)]\n",
    "                )\n",
    "            if train_acc is not None:\n",
    "                train_accs = (\n",
    "                    [train_acc.item()]\n",
    "                    if hasattr(train_acc, \"item\")\n",
    "                    else [float(train_acc)]\n",
    "                )\n",
    "            if val_acc is not None:\n",
    "                val_accs = (\n",
    "                    [val_acc.item()] if hasattr(val_acc, \"item\") else [float(val_acc)]\n",
    "                )\n",
    "\n",
    "            epochs = list(range(1, len(train_losses) + 1)) if train_losses else [1]\n",
    "\n",
    "        # Method 2: Try to extract from logger experiment\n",
    "        elif hasattr(trainer.logger, \"experiment\"):\n",
    "            print(\"üìä Extracting metrics from logger experiment...\")\n",
    "            # This would work with TensorBoard or other loggers that store history\n",
    "            # For now, we'll use the final epoch values\n",
    "            pass\n",
    "\n",
    "        # Create a figure with two subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        # Plot accuracy if we have data\n",
    "        if train_accs or val_accs:\n",
    "            if train_accs:\n",
    "                ax1.plot(\n",
    "                    epochs[: len(train_accs)],\n",
    "                    train_accs,\n",
    "                    \"b-\",\n",
    "                    label=\"Training Accuracy\",\n",
    "                    linewidth=2,\n",
    "                    marker=\"o\",\n",
    "                )\n",
    "            if val_accs:\n",
    "                ax1.plot(\n",
    "                    epochs[: len(val_accs)],\n",
    "                    val_accs,\n",
    "                    \"r-\",\n",
    "                    label=\"Validation Accuracy\",\n",
    "                    linewidth=2,\n",
    "                    marker=\"s\",\n",
    "                )\n",
    "            ax1.legend()\n",
    "            ax1.set_ylabel(\"Accuracy\")\n",
    "        else:\n",
    "            # Show final accuracy values if available\n",
    "            final_train_acc = callbacks.get(\n",
    "                \"train_acc_epoch\", callbacks.get(\"train_acc\", None)\n",
    "            )\n",
    "            final_val_acc = callbacks.get(\"val_acc\", None)\n",
    "\n",
    "            ax1.text(\n",
    "                0.5,\n",
    "                0.6,\n",
    "                (\n",
    "                    f\"üìä Final Training Accuracy: {final_train_acc:.3f}\"\n",
    "                    if final_train_acc\n",
    "                    else \"üìä Training Accuracy\"\n",
    "                ),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax1.transAxes,\n",
    "                fontsize=11,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "            ax1.text(\n",
    "                0.5,\n",
    "                0.4,\n",
    "                (\n",
    "                    f\"üìä Final Validation Accuracy: {final_val_acc:.3f}\"\n",
    "                    if final_val_acc\n",
    "                    else \"üìä Validation Accuracy\"\n",
    "                ),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax1.transAxes,\n",
    "                fontsize=11,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "        ax1.set_title(\"Model Accuracy Over Time\", fontweight=\"bold\", fontsize=14)\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot loss if we have data\n",
    "        if train_losses or val_losses:\n",
    "            if train_losses:\n",
    "                ax2.plot(\n",
    "                    epochs[: len(train_losses)],\n",
    "                    train_losses,\n",
    "                    \"b-\",\n",
    "                    label=\"Training Loss\",\n",
    "                    linewidth=2,\n",
    "                    marker=\"o\",\n",
    "                )\n",
    "            if val_losses:\n",
    "                ax2.plot(\n",
    "                    epochs[: len(val_losses)],\n",
    "                    val_losses,\n",
    "                    \"r-\",\n",
    "                    label=\"Validation Loss\",\n",
    "                    linewidth=2,\n",
    "                    marker=\"s\",\n",
    "                )\n",
    "            ax2.legend()\n",
    "            ax2.set_ylabel(\"Loss\")\n",
    "        else:\n",
    "            # Show final loss values if available\n",
    "            final_train_loss = callbacks.get(\n",
    "                \"train_loss_epoch\", callbacks.get(\"train_loss\", None)\n",
    "            )\n",
    "            final_val_loss = callbacks.get(\"val_loss\", None)\n",
    "\n",
    "            ax2.text(\n",
    "                0.5,\n",
    "                0.6,\n",
    "                (\n",
    "                    f\"üìâ Final Training Loss: {final_train_loss:.3f}\"\n",
    "                    if final_train_loss\n",
    "                    else \"üìâ Training Loss\"\n",
    "                ),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax2.transAxes,\n",
    "                fontsize=11,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "            ax2.text(\n",
    "                0.5,\n",
    "                0.4,\n",
    "                (\n",
    "                    f\"üìâ Final Validation Loss: {final_val_loss:.3f}\"\n",
    "                    if final_val_loss\n",
    "                    else \"üìâ Validation Loss\"\n",
    "                ),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax2.transAxes,\n",
    "                fontsize=11,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "        ax2.set_title(\"Model Loss Over Time\", fontweight=\"bold\", fontsize=14)\n",
    "        ax2.set_xlabel(\"Epoch\")\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"üìà What these metrics tell us:\")\n",
    "        print(\"   ‚úÖ Higher accuracy = better performance\")\n",
    "        print(\"   ‚úÖ Lower loss = fewer mistakes\")\n",
    "        print(\"   ‚úÖ Similar train/val curves = good generalization\")\n",
    "\n",
    "        # Print final metrics summary\n",
    "        print(\"\\nüéØ Final Performance Summary:\")\n",
    "        if callbacks:\n",
    "            for metric_name, value in callbacks.items():\n",
    "                if (\n",
    "                    any(key in metric_name for key in [\"acc\", \"loss\"])\n",
    "                    and value is not None\n",
    "                ):\n",
    "                    val = value.item() if hasattr(value, \"item\") else value\n",
    "                    if \"acc\" in metric_name:\n",
    "                        print(f\"   üìä {metric_name}: {val:.1%}\")\n",
    "                    else:\n",
    "                        print(f\"   üìâ {metric_name}: {val:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not extract detailed training history: {e}\")\n",
    "        print(\"üìä This can happen with different logger configurations.\")\n",
    "        print(\n",
    "            \"üìà The model still trained successfully - check the training logs above!\"\n",
    "        )\n",
    "\n",
    "        # Create a simple summary plot\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"üìà Training Completed Successfully!\\n\\n\"\n",
    "            + \"üìä Check the progress bars above for metrics\\n\"\n",
    "            + \"üéØ Model performance logged during training\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax.set_title(\"Training Summary\", fontweight=\"bold\", fontsize=16)\n",
    "        ax.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nldt4Ez1UIjl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRACTICAL DEMO: Using our model for real-time emotion recognition\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def load_trained_model(model_path=\"emotion_recognition_model.pth\"):\n",
    "    \"\"\"\n",
    "    Load our saved emotion recognition model\n",
    "    This shows how we'll use the model in our game!\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üîÑ Loading trained emotion recognition model...\")\n",
    "\n",
    "    # Load the saved model data\n",
    "    checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "    # Recreate the model architecture\n",
    "    loaded_model = EmotionCNN(\n",
    "        num_classes=checkpoint[\"num_classes\"],\n",
    "        learning_rate=checkpoint[\"model_config\"][\"learning_rate\"],\n",
    "    )\n",
    "\n",
    "    # Load the learned weights\n",
    "    loaded_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    loaded_model.eval()  # Set to evaluation mode\n",
    "\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    return loaded_model, checkpoint[\"class_names\"]\n",
    "\n",
    "\n",
    "def predict_emotion_from_image(model, image_array, class_names):\n",
    "    \"\"\"\n",
    "    Predict emotion from a single image\n",
    "    This is how our game will analyze player expressions!\n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess the image (same as training)\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((80, 80)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Convert image and add batch dimension\n",
    "    if isinstance(image_array, np.ndarray):\n",
    "        # Convert numpy array to PIL Image\n",
    "        from PIL import Image\n",
    "\n",
    "        if len(image_array.shape) == 3:\n",
    "            image_array = np.mean(image_array, axis=2)  # Convert to grayscale\n",
    "        pil_image = Image.fromarray((image_array * 255).astype(np.uint8))\n",
    "        tensor_image = transform(pil_image).unsqueeze(0)\n",
    "    else:\n",
    "        tensor_image = transform(image_array).unsqueeze(0)\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensor_image)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "\n",
    "        predicted_emotion = class_names[predicted_idx.item()]\n",
    "        confidence_score = confidence.item()\n",
    "\n",
    "    return predicted_emotion, confidence_score, probabilities[0].numpy()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GAME INTEGRATION EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üéÆ GAME INTEGRATION EXAMPLE\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Here's how our emotion recognition will work in the game:\")\n",
    "\n",
    "# Simulate loading the model (as it would happen in the game)\n",
    "try:\n",
    "    if os.path.exists(\"emotion_recognition_model.pth\"):\n",
    "        game_model, game_class_names = load_trained_model()\n",
    "\n",
    "        print(f\"üéØ Model ready with emotions: {game_class_names}\")\n",
    "\n",
    "        # Example of how the game would use this\n",
    "        print(\"\\nüéÆ Game Integration Workflow:\")\n",
    "        print(\"   1. üì∑ Capture player's face from webcam\")\n",
    "        print(\"   2. üñºÔ∏è  Preprocess image (resize, normalize)\")\n",
    "        print(\"   3. üß† Run through emotion recognition model\")\n",
    "        print(\"   4. üé≠ Get emotion prediction + confidence\")\n",
    "        print(\"   5. üí¨ Adjust NPC dialogue based on emotion\")\n",
    "        print(\"   6. üîÑ Repeat in real-time during gameplay\")\n",
    "\n",
    "        # Simulate game dialogue logic\n",
    "        def generate_npc_response(detected_emotion, confidence):\n",
    "            \"\"\"\n",
    "            Example of how NPCs might respond to player emotions\n",
    "            \"\"\"\n",
    "            responses = {\n",
    "                \"happy\": [\n",
    "                    \"Great to see you smiling! Let me help you with something special.\",\n",
    "                    \"Your happiness is contagious! Here's a bonus for you.\",\n",
    "                ],\n",
    "                \"sad\": [\n",
    "                    \"I can see you're feeling down. Is there anything I can do to help?\",\n",
    "                    \"Don't worry, things will get better. Here's something to cheer you up.\",\n",
    "                ],\n",
    "                \"angry\": [\n",
    "                    \"I can see you're frustrated. Let's work together to solve this.\",\n",
    "                    \"Take a deep breath. I'm here to help, not make things worse.\",\n",
    "                ],\n",
    "                \"surprise\": [\n",
    "                    \"Oh! You look surprised! Did something unexpected happen?\",\n",
    "                    \"That's quite a reaction! What caught you off guard?\",\n",
    "                ],\n",
    "                \"fear\": [\n",
    "                    \"You seem worried. Don't be afraid, I'm here to help you.\",\n",
    "                    \"Is something troubling you? Let's face it together.\",\n",
    "                ],\n",
    "                \"neutral\": [\n",
    "                    \"Hello there! How can I assist you today?\",\n",
    "                    \"Welcome! What brings you to see me?\",\n",
    "                ],\n",
    "            }\n",
    "\n",
    "            if confidence > 0.7:  # High confidence\n",
    "                return f\"[CONFIDENT] {responses.get(detected_emotion, responses['neutral'])[0]}\"\n",
    "            else:  # Lower confidence\n",
    "                return f\"[UNCERTAIN] {responses.get('neutral')[0]}\"\n",
    "\n",
    "        # Example usage\n",
    "        print(\"\\nüé≠ Example NPC Responses:\")\n",
    "        for emotion in game_class_names:\n",
    "            response = generate_npc_response(emotion, 0.85)\n",
    "            print(f\"   {emotion.upper()}: {response}\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Model file not found. Train the model first!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS FOR STUDENTS:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"1. üß™ Experiment with different emotions in front of a camera\")\n",
    "print(\"2. üéÆ Integrate this model into the provided game framework\")\n",
    "print(\"3. üí¨ Create more sophisticated NPC dialogue trees\")\n",
    "print(\"4. üé® Add visual feedback when emotions are detected\")\n",
    "print(\"5. üìä Collect data on how players respond to emotion-aware NPCs\")\n",
    "\n",
    "print(\"\\nüéì LEARNING OBJECTIVES ACHIEVED:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"‚úÖ Understanding of Computer Vision concepts\")\n",
    "print(\"‚úÖ Hands-on experience with Deep Learning\")\n",
    "print(\"‚úÖ Practical application of AI in gaming\")\n",
    "print(\"‚úÖ Real-time emotion recognition system\")\n",
    "print(\"‚úÖ Integration of AI with interactive applications\")\n",
    "\n",
    "print(\"\\nüåü CONGRATULATIONS!\")\n",
    "print(\"You've successfully built an AI system that can:\")\n",
    "print(\"   üß† Understand human emotions from facial expressions\")\n",
    "print(\"   üéÆ Enhance gaming experiences with emotional intelligence\")\n",
    "print(\"   üî¨ Apply cutting-edge computer vision technology\")\n",
    "print(\"   üöÄ Create more engaging and responsive applications\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéä WELCOME TO THE FUTURE OF EMOTION-AWARE TECHNOLOGY! üéä\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NGC-PyTorch-2.3",
   "language": "python",
   "name": "ngc-pytorch-2.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
