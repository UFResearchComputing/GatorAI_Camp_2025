{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UFResearchComputing/gatorAI_summer_camp_2024/blob/main/01_full_of_emotion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><img src=\"images/gator_ai_camp_2024_logo_200.png\" align=\"right\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KhpJb5VI79P"
      },
      "source": [
        "# Gator AI Summer Camp 2025\n",
        "\n",
        "In this notebook, we're going to use Python to create a deep learning model that can take images of faces and output the emotion being expressed.\n",
        "\n",
        "The dataset we're going to use is the FER-2013 dataset, which contains 35,887 grayscale images of faces. Each image is 48x48 pixels and is labeled with one of seven emotions: anger, disgust, fear, happiness, sadness, surprise, or neutral. The dataset and more information can be found [on Kaggle](https://www.kaggle.com/datasets/msambare/fer2013/data).\n",
        "\n",
        "**Note:** One issue with the dataset is that it has relatively few images in the disgust category, so we drop that category for this exercise.\n",
        "\n",
        "To build our model, we'll use the Keras deep learning library, which provides a high-level interface for building and training neural networks. We'll start by loading the dataset and exploring the images, then we'll build and train a convolutional neural network (CNN) to classify the emotions in the images.\n",
        "\n",
        "**Before you get started, make sure to select a Runtime with a GPU!** <img src='images/colab_change_runtime_type.png' align='right' width='50%' alt='Image of the Runtime menu options in Google Colab'>\n",
        "* Go to the **\"Runtime\"** menu\n",
        "* Select **\"Change runtime type\"**\n",
        "* Select **\"T4 GPU\"** and click **\"Save\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Learning Objectives & What You'll Build\n",
        "\n",
        "## üß† What is Computer Vision?\n",
        "Computer Vision is a field of AI that teaches computers to \"see\" and understand images, just like humans do! In this notebook, you'll build a system that can look at a person's face and automatically detect their emotion.\n",
        "\n",
        "## üéÆ Real-World Application: Emotion-Aware Gaming\n",
        "The emotion recognition model you'll create will be integrated into our adventure game, allowing Non-Player Characters (NPCs) to respond differently based on your facial expressions. Imagine:\n",
        "- **Sad expression** ‚Üí NPCs offer comfort and help\n",
        "- **Happy expression** ‚Üí NPCs share in your joy and give bonuses  \n",
        "- **Angry expression** ‚Üí NPCs try to calm you down\n",
        "- **Surprised expression** ‚Üí NPCs react to your amazement\n",
        "\n",
        "## üìö What You'll Learn Today\n",
        "\n",
        "### üî¨ **Computer Vision Concepts**\n",
        "- How computers \"see\" and process images\n",
        "- What makes facial expressions recognizable\n",
        "- Image preprocessing and data augmentation\n",
        "\n",
        "### üß† **Deep Learning Fundamentals**\n",
        "- **Convolutional Neural Networks (CNNs)** - the AI architecture that powers image recognition\n",
        "- **Training Process** - how AI learns from thousands of examples\n",
        "- **Model Evaluation** - measuring how well our AI performs\n",
        "\n",
        "### üõ†Ô∏è **Practical Skills**\n",
        "- Using **PyTorch Lightning** for efficient deep learning\n",
        "- Working with real-world datasets (FER-2013 emotion dataset)\n",
        "- Visualizing model performance and debugging\n",
        "- Saving and loading trained models for deployment\n",
        "\n",
        "### üéÆ **Game Integration**\n",
        "- Loading pre-trained models in applications\n",
        "- Real-time emotion detection from camera input\n",
        "- Creating responsive NPC behavior based on emotions\n",
        "\n",
        "## üó∫Ô∏è Our Journey Today\n",
        "\n",
        "1. **üìä Data Exploration** - Understand our emotion dataset\n",
        "2. **üèóÔ∏è Model Architecture** - Build our CNN emotion detector  \n",
        "3. **üéì Training Process** - Teach our AI to recognize emotions\n",
        "4. **üìà Evaluation** - Test how well our model performs\n",
        "5. **üíæ Model Saving** - Prepare our model for the game\n",
        "6. **üéÆ Game Integration** - See how it works in practice\n",
        "\n",
        "## üöÄ By the End of This Notebook\n",
        "\n",
        "You'll have created a complete emotion recognition system that can:\n",
        "- ‚úÖ Detect 6 different emotions from facial expressions\n",
        "- ‚úÖ Work in real-time with camera input\n",
        "- ‚úÖ Integrate seamlessly with our adventure game\n",
        "- ‚úÖ Provide the foundation for emotion-aware applications\n",
        "\n",
        "**Let's build the future of emotionally intelligent technology!** üåü"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVz8_6AYI79Q",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORT LIBRARIES: The tools we need to build our emotion recognition system\n",
        "# =============================================================================\n",
        "\n",
        "# Basic Python libraries for file handling and data manipulation\n",
        "import os                    # For working with files and directories\n",
        "import sys                   # For system-specific operations\n",
        "import shutil                # For copying and moving files\n",
        "import zipfile               # For extracting zip archives\n",
        "import random                # For generating random numbers\n",
        "import pandas as pd          # For handling data in table format\n",
        "import numpy as np           # For mathematical operations and arrays\n",
        "import matplotlib.pyplot as plt  # For creating graphs and visualizations\n",
        "from tqdm.auto import tqdm  # Progress bar library\n",
        "import time\n",
        "\n",
        "# Display plots directly in the notebook\n",
        "%matplotlib inline           \n",
        "\n",
        "# Additional utilities\n",
        "from functools import reduce\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import kagglehub            # For downloading datasets from Kaggle\n",
        "\n",
        "# =============================================================================\n",
        "# PYTORCH LIGHTNING: Our main deep learning framework\n",
        "# =============================================================================\n",
        "# PyTorch Lightning makes it easier to organize and train neural networks\n",
        "# It handles a lot of the complex training logic for us!\n",
        "\n",
        "import torch                           # Core PyTorch library\n",
        "import torch.nn as nn                  # Neural network building blocks\n",
        "import torch.nn.functional as F        # Common neural network functions\n",
        "import torchvision                     # Computer vision utilities\n",
        "import torchvision.transforms as transforms  # Image preprocessing tools\n",
        "from torch.utils.data import DataLoader, Dataset  # Data loading utilities\n",
        "\n",
        "import pytorch_lightning as pl         # Lightning framework for easier training\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "# =============================================================================\n",
        "# EVALUATION TOOLS: How we measure our model's performance\n",
        "# =============================================================================\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(\"üß† Ready to build an emotion recognition system!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the dataset using kagglehub\n",
        "print(\"Downloading dataset from Kaggle...\")\n",
        "dataset_path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
        "print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.makedirs(\"data\")\n",
        "    print(\"Created 'data' directory\")\n",
        "\n",
        "# Check what files/folders are in the downloaded dataset\n",
        "print(f\"Contents of {dataset_path}:\")\n",
        "for item in os.listdir(dataset_path):\n",
        "    item_path = os.path.join(dataset_path, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        print(f\"  Directory: {item}\")\n",
        "    else:\n",
        "        print(f\"  File: {item}\")\n",
        "\n",
        "# Look for zip file first\n",
        "zip_file = None\n",
        "for file in os.listdir(dataset_path):\n",
        "    if file.endswith(\".zip\"):\n",
        "        zip_file = os.path.join(dataset_path, file)\n",
        "        break\n",
        "\n",
        "if zip_file:\n",
        "    # Extract the zip file to the data directory\n",
        "    print(f\"Extracting {zip_file} to data/\")\n",
        "    with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\"data/\")\n",
        "    print(\"Extraction complete\")\n",
        "else:\n",
        "    # No zip file found, check if train/test directories already exist\n",
        "    train_dir = os.path.join(dataset_path, \"train\")\n",
        "    test_dir = os.path.join(dataset_path, \"test\")\n",
        "\n",
        "    if os.path.exists(train_dir) and os.path.exists(test_dir):\n",
        "        print(\"Found train and test directories, copying to data/\")\n",
        "        # Copy the train and test directories\n",
        "        shutil.copytree(train_dir, os.path.join(\"data\", \"train\"), dirs_exist_ok=True)\n",
        "        shutil.copytree(test_dir, os.path.join(\"data\", \"test\"), dirs_exist_ok=True)\n",
        "        print(\"Directories copied successfully\")\n",
        "    else:\n",
        "        # Look for any other structure\n",
        "        print(\"Searching for dataset files in subdirectories...\")\n",
        "        for root, dirs, files in os.walk(dataset_path):\n",
        "            if \"train\" in dirs and \"test\" in dirs:\n",
        "                train_source = os.path.join(root, \"train\")\n",
        "                test_source = os.path.join(root, \"test\")\n",
        "                print(f\"Found train/test directories in: {root}\")\n",
        "                shutil.copytree(\n",
        "                    train_source, os.path.join(\"data\", \"train\"), dirs_exist_ok=True\n",
        "                )\n",
        "                shutil.copytree(\n",
        "                    test_source, os.path.join(\"data\", \"test\"), dirs_exist_ok=True\n",
        "                )\n",
        "                print(\"Directories copied successfully\")\n",
        "                break\n",
        "        else:\n",
        "            print(\"Could not find train/test directories in the downloaded dataset\")\n",
        "\n",
        "# Verify the data directory structure\n",
        "if os.path.exists(\"data\"):\n",
        "    print(f\"\\nContents of data directory:\")\n",
        "    for item in os.listdir(\"data\"):\n",
        "        item_path = os.path.join(\"data\", item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"  Directory: {item}\")\n",
        "            # Show subdirectories (emotion categories)\n",
        "            if os.path.exists(item_path):\n",
        "                subdirs = [\n",
        "                    d\n",
        "                    for d in os.listdir(item_path)\n",
        "                    if os.path.isdir(os.path.join(item_path, d))\n",
        "                ]\n",
        "                print(f\"    Emotion categories: {subdirs}\")\n",
        "\n",
        "# Delete the disgust folders as there are so few examples in that category\n",
        "disgust_train_path = os.path.join(\"data\", \"train\", \"disgust\")\n",
        "disgust_test_path = os.path.join(\"data\", \"test\", \"disgust\")\n",
        "\n",
        "if os.path.exists(disgust_train_path):\n",
        "    shutil.rmtree(disgust_train_path)\n",
        "    print(\"Removed data/train/disgust folder\")\n",
        "\n",
        "if os.path.exists(disgust_test_path):\n",
        "    shutil.rmtree(disgust_test_path)\n",
        "    print(\"Removed data/test/disgust folder\")\n",
        "\n",
        "print(\"Dataset preparation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ioyd0sAI79R",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CUSTOM DATASET CLASS: Teaching the computer how to read our emotion images\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This class teaches PyTorch how to load and process our emotion images.\n",
        "    Think of it as a recipe that tells the computer:\n",
        "    1. Where to find each image\n",
        "    2. What emotion label goes with each image\n",
        "    3. How to prepare the image for our neural network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize our dataset\n",
        "        - root_dir: folder containing our emotion images\n",
        "        - transform: image preprocessing steps (resize, normalize, etc.)\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.class_names = []\n",
        "\n",
        "        # Find all emotion categories (angry, happy, sad, etc.)\n",
        "        emotion_folders = sorted(\n",
        "            [\n",
        "                f\n",
        "                for f in os.listdir(root_dir)\n",
        "                if os.path.isdir(os.path.join(root_dir, f))\n",
        "            ]\n",
        "        )\n",
        "        self.class_names = emotion_folders\n",
        "\n",
        "        print(f\"üìÇ Found emotion categories: {self.class_names}\")\n",
        "\n",
        "        # Load all image paths and their corresponding emotion labels\n",
        "        for label_idx, emotion in enumerate(emotion_folders):\n",
        "            emotion_path = os.path.join(root_dir, emotion)\n",
        "\n",
        "            # Count images in this emotion category\n",
        "            image_files = [\n",
        "                f\n",
        "                for f in os.listdir(emotion_path)\n",
        "                if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "            ]\n",
        "            print(f\"   {emotion}: {len(image_files)} images\")\n",
        "\n",
        "            # Add each image path and its label to our lists\n",
        "            for img_file in image_files:\n",
        "                self.images.append(os.path.join(emotion_path, img_file))\n",
        "                self.labels.append(label_idx)\n",
        "\n",
        "        print(f\"üìä Total dataset size: {len(self.images)} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the total number of images in our dataset\"\"\"\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single image and its label\n",
        "        This function is called each time we want to train on one image\n",
        "        \"\"\"\n",
        "        # Load the image\n",
        "        img_path = self.images[idx]\n",
        "        image = plt.imread(img_path)\n",
        "\n",
        "        # Convert to grayscale if needed (emotion recognition works well with grayscale)\n",
        "        if len(image.shape) == 3:\n",
        "            image = np.mean(image, axis=2)\n",
        "\n",
        "        # Convert to PyTorch tensor format\n",
        "        image = torch.FloatTensor(image).unsqueeze(0)  # Add channel dimension\n",
        "\n",
        "        # Apply preprocessing transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# PYTORCH LIGHTNING DATA MODULE: Organizing our data for training\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "class EmotionDataModule(pl.LightningDataModule):\n",
        "    \"\"\"\n",
        "    This class handles all our data loading and preprocessing.\n",
        "    Lightning Data Modules keep our data code organized and reusable!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, batch_size=32, img_size=80, num_workers=4):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "        # Define image preprocessing steps\n",
        "        # These help our model work better and prevent overfitting\n",
        "        self.train_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(\n",
        "                    (img_size, img_size)\n",
        "                ),  # Resize all images to same size\n",
        "                transforms.RandomHorizontalFlip(\n",
        "                    p=0.5\n",
        "                ),  # Randomly flip images (data augmentation)\n",
        "                transforms.RandomRotation(\n",
        "                    degrees=10\n",
        "                ),  # Randomly rotate images slightly\n",
        "                transforms.ColorJitter(\n",
        "                    brightness=0.2, contrast=0.2\n",
        "                ),  # Adjust brightness/contrast\n",
        "                transforms.Normalize(mean=[0.5], std=[0.5]),  # Normalize pixel values\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.val_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(\n",
        "                    (img_size, img_size)\n",
        "                ),  # Resize (no randomness for validation)\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.5], std=[0.5]\n",
        "                ),  # Same normalization as training\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"Load our training and validation datasets\"\"\"\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            # Create training dataset\n",
        "            train_dir = os.path.join(self.data_dir, \"train\")\n",
        "            self.train_dataset = EmotionDataset(\n",
        "                train_dir, transform=self.train_transform\n",
        "            )\n",
        "\n",
        "            # Create validation dataset\n",
        "            val_dir = os.path.join(self.data_dir, \"test\")\n",
        "            self.val_dataset = EmotionDataset(val_dir, transform=self.val_transform)\n",
        "\n",
        "            # Store class names for later use\n",
        "            self.class_names = self.train_dataset.class_names\n",
        "            self.num_classes = len(self.class_names)\n",
        "\n",
        "            print(f\"üéØ Number of emotion classes: {self.num_classes}\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"Create the data loader for training\"\"\"\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,  # Randomize order for better training\n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\"Create the data loader for validation\"\"\"\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,  # Don't randomize validation data\n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# INITIALIZE OUR DATA MODULE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üèóÔ∏è  Setting up our emotion dataset...\")\n",
        "data_path = \"data/\"\n",
        "data_module = EmotionDataModule(data_dir=data_path, batch_size=32, img_size=80)\n",
        "data_module.setup()\n",
        "\n",
        "print(f\"‚úÖ Data module ready!\")\n",
        "print(f\"üìä Training images: {len(data_module.train_dataset)}\")\n",
        "print(f\"üìä Validation images: {len(data_module.val_dataset)}\")\n",
        "print(f\"üòä Emotion categories: {data_module.class_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXPLORE OUR DATA: Let's see what we're working with!\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def visualize_sample_images_simple(data_dir=\"data/\", num_samples=3):\n",
        "    \"\"\"\n",
        "    Simple and robust data visualization that reads files directly\n",
        "    This avoids DataLoader issues that can cause hanging\n",
        "    \"\"\"\n",
        "    print(\"üñºÔ∏è  Sample images from each emotion category:\")\n",
        "    print(\"üìÅ Reading files directly from disk...\")\n",
        "\n",
        "    train_dir = os.path.join(data_dir, \"train\")\n",
        "    test_dir = os.path.join(data_dir, \"test\")\n",
        "\n",
        "    if not os.path.exists(train_dir):\n",
        "        print(f\"‚ùå Training directory not found: {train_dir}\")\n",
        "        return\n",
        "\n",
        "    # Get emotion categories\n",
        "    emotion_categories = [\n",
        "        d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))\n",
        "    ]\n",
        "    emotion_categories.sort()\n",
        "\n",
        "    print(\n",
        "        f\"üòä Found {len(emotion_categories)} emotion categories: {emotion_categories}\"\n",
        "    )\n",
        "\n",
        "    # Collect sample images and count all images\n",
        "    sample_images = {}\n",
        "    train_counts = {}\n",
        "    test_counts = {}\n",
        "\n",
        "    print(\"\\nüìä Collecting sample images and counting files...\")\n",
        "\n",
        "    for emotion in emotion_categories:\n",
        "        print(f\"   Processing {emotion}...\")\n",
        "\n",
        "        # Training data\n",
        "        train_emotion_dir = os.path.join(train_dir, emotion)\n",
        "        train_files = [\n",
        "            f\n",
        "            for f in os.listdir(train_emotion_dir)\n",
        "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "        ]\n",
        "        train_counts[emotion] = len(train_files)\n",
        "\n",
        "        # Test data\n",
        "        test_emotion_dir = os.path.join(test_dir, emotion)\n",
        "        test_files = []\n",
        "        if os.path.exists(test_emotion_dir):\n",
        "            test_files = [\n",
        "                f\n",
        "                for f in os.listdir(test_emotion_dir)\n",
        "                if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "            ]\n",
        "        test_counts[emotion] = len(test_files)\n",
        "\n",
        "        # Sample a few images for visualization\n",
        "        sample_files = random.sample(train_files, min(num_samples, len(train_files)))\n",
        "        sample_images[emotion] = []\n",
        "\n",
        "        for sample_file in sample_files:\n",
        "            try:\n",
        "                img_path = os.path.join(train_emotion_dir, sample_file)\n",
        "                # Read image using PIL (more reliable than matplotlib for various formats)\n",
        "                with Image.open(img_path) as img:\n",
        "                    # Convert to grayscale and numpy array\n",
        "                    img_gray = img.convert(\"L\")\n",
        "                    img_array = np.array(img_gray)\n",
        "                    sample_images[emotion].append(img_array)\n",
        "            except Exception as e:\n",
        "                print(f\"      ‚ö†Ô∏è  Could not load {sample_file}: {e}\")\n",
        "\n",
        "    print(\"‚úÖ Sample collection complete!\")\n",
        "\n",
        "    # Display sample images\n",
        "    print(\"\\nüé® Creating sample image visualization...\")\n",
        "\n",
        "    try:\n",
        "        fig, axes = plt.subplots(\n",
        "            len(emotion_categories),\n",
        "            num_samples,\n",
        "            figsize=(15, 2.5 * len(emotion_categories)),\n",
        "        )\n",
        "        fig.suptitle(\n",
        "            \"Sample Images from Each Emotion Category\", fontsize=16, fontweight=\"bold\"\n",
        "        )\n",
        "\n",
        "        for i, emotion in enumerate(emotion_categories):\n",
        "            for j in range(num_samples):\n",
        "                if len(emotion_categories) > 1:\n",
        "                    ax = axes[i, j]\n",
        "                else:\n",
        "                    ax = axes[j]\n",
        "\n",
        "                if j < len(sample_images[emotion]):\n",
        "                    ax.imshow(sample_images[emotion][j], cmap=\"gray\")\n",
        "                    ax.set_title(f\"{emotion.title()}\", fontsize=10)\n",
        "                else:\n",
        "                    ax.text(\n",
        "                        0.5,\n",
        "                        0.5,\n",
        "                        \"No image\",\n",
        "                        ha=\"center\",\n",
        "                        va=\"center\",\n",
        "                        transform=ax.transAxes,\n",
        "                        fontsize=10,\n",
        "                    )\n",
        "                    ax.set_title(f\"{emotion.title()}\", fontsize=10)\n",
        "\n",
        "                ax.axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print(\"‚úÖ Sample images displayed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating sample visualization: {e}\")\n",
        "        print(\"üìä Showing text summary instead...\")\n",
        "\n",
        "        for emotion in emotion_categories:\n",
        "            print(f\"   {emotion}: {len(sample_images[emotion])} samples loaded\")\n",
        "\n",
        "    # Display class distribution\n",
        "    print(\"\\nüìà Creating class distribution charts...\")\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(14, 6))\n",
        "\n",
        "        # Training data distribution\n",
        "        plt.subplot(1, 2, 1)\n",
        "        emotions = list(train_counts.keys())\n",
        "        train_values = list(train_counts.values())\n",
        "\n",
        "        bars = plt.bar(\n",
        "            emotions, train_values, color=\"skyblue\", edgecolor=\"navy\", alpha=0.8\n",
        "        )\n",
        "        plt.title(\"Training Data Distribution\", fontweight=\"bold\", fontsize=14)\n",
        "        plt.xlabel(\"Emotion Category\", fontweight=\"bold\")\n",
        "        plt.ylabel(\"Number of Images\", fontweight=\"bold\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "\n",
        "        # Add count labels on bars\n",
        "        for bar, count in zip(bars, train_values):\n",
        "            plt.text(\n",
        "                bar.get_x() + bar.get_width() / 2,\n",
        "                bar.get_height() + max(train_values) * 0.01,\n",
        "                str(count),\n",
        "                ha=\"center\",\n",
        "                fontweight=\"bold\",\n",
        "                fontsize=10,\n",
        "            )\n",
        "\n",
        "        # Test/Validation data distribution\n",
        "        plt.subplot(1, 2, 2)\n",
        "        test_values = list(test_counts.values())\n",
        "\n",
        "        bars = plt.bar(\n",
        "            emotions, test_values, color=\"lightcoral\", edgecolor=\"darkred\", alpha=0.8\n",
        "        )\n",
        "        plt.title(\"Test/Validation Data Distribution\", fontweight=\"bold\", fontsize=14)\n",
        "        plt.xlabel(\"Emotion Category\", fontweight=\"bold\")\n",
        "        plt.ylabel(\"Number of Images\", fontweight=\"bold\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "\n",
        "        # Add count labels on bars\n",
        "        for bar, count in zip(bars, test_values):\n",
        "            plt.text(\n",
        "                bar.get_x() + bar.get_width() / 2,\n",
        "                bar.get_height() + max(test_values) * 0.01,\n",
        "                str(count),\n",
        "                ha=\"center\",\n",
        "                fontweight=\"bold\",\n",
        "                fontsize=10,\n",
        "            )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print(\"‚úÖ Distribution charts created successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating distribution charts: {e}\")\n",
        "        print(\"üìä Showing numerical summary:\")\n",
        "\n",
        "        print(\"\\nüìä Dataset Summary:\")\n",
        "        print(\"=\" * 50)\n",
        "        total_train = sum(train_counts.values())\n",
        "        total_test = sum(test_counts.values())\n",
        "\n",
        "        for emotion in emotions:\n",
        "            train_count = train_counts[emotion]\n",
        "            test_count = test_counts[emotion]\n",
        "            train_pct = (train_count / total_train * 100) if total_train > 0 else 0\n",
        "            test_pct = (test_count / total_test * 100) if total_test > 0 else 0\n",
        "\n",
        "            print(\n",
        "                f\"{emotion.ljust(12)}: {train_count:>6} train ({train_pct:5.1f}%) | {test_count:>6} test ({test_pct:5.1f}%)\"\n",
        "            )\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        print(\n",
        "            f\"{'TOTAL'.ljust(12)}: {total_train:>6} train (100.0%) | {total_test:>6} test (100.0%)\"\n",
        "        )\n",
        "\n",
        "    print(\"\\nüìà What do these distributions tell us?\")\n",
        "    print(\"   ‚úì Some emotions have more examples than others (class imbalance)\")\n",
        "    print(\"   ‚úì This is normal in real-world datasets\")\n",
        "    print(\"   ‚úì We'll use techniques like class weighting to handle this\")\n",
        "    print(\"   ‚úì Each emotion shows different facial patterns we need to learn\")\n",
        "\n",
        "    print(\"\\n‚úÖ Data exploration complete!\")\n",
        "    print(\"üéØ Ready to build our emotion recognition model!\")\n",
        "\n",
        "\n",
        "# Run the simplified visualization\n",
        "print(\"üöÄ Starting robust data exploration...\")\n",
        "\n",
        "try:\n",
        "    visualize_sample_images_simple(\"data/\", num_samples=3)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Visualization failed: {e}\")\n",
        "    print(\"üìã Let's check what we have in the data directory...\")\n",
        "\n",
        "    if os.path.exists(\"data\"):\n",
        "        print(\"üìÅ Contents of data directory:\")\n",
        "        for item in os.listdir(\"data\"):\n",
        "            item_path = os.path.join(\"data\", item)\n",
        "            if os.path.isdir(item_path):\n",
        "                print(f\"   üìÇ {item}/\")\n",
        "                try:\n",
        "                    sub_items = os.listdir(item_path)[:5]  # Show first 5 items\n",
        "                    for sub_item in sub_items:\n",
        "                        print(f\"      üìÑ {sub_item}\")\n",
        "                    if len(os.listdir(item_path)) > 5:\n",
        "                        print(\n",
        "                            f\"      ... and {len(os.listdir(item_path)) - 5} more files\"\n",
        "                        )\n",
        "                except:\n",
        "                    print(f\"      ‚ùå Could not read directory\")\n",
        "            else:\n",
        "                print(f\"   üìÑ {item}\")\n",
        "    else:\n",
        "        print(\"‚ùå Data directory does not exist!\")\n",
        "        print(\"üí° Please run the data download cell (cell 5) first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnhLgZuKI79S",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONVOLUTIONAL NEURAL NETWORK (CNN): Our emotion recognition brain!\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "class EmotionCNN(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    This is our Convolutional Neural Network (CNN) for emotion recognition.\n",
        "    Think of it as a digital brain that learns to recognize patterns in faces!\n",
        "\n",
        "    How CNNs work for emotion recognition:\n",
        "    1. CONVOLUTION layers detect features (edges, shapes, textures)\n",
        "    2. POOLING layers reduce image size while keeping important info\n",
        "    3. DROPOUT layers prevent overfitting (memorizing instead of learning)\n",
        "    4. DENSE layers make the final emotion decision\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=6, learning_rate=0.001, class_weights=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store hyperparameters (settings we can adjust)\n",
        "        self.save_hyperparameters()\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "        # =================================================================\n",
        "        # FEATURE EXTRACTION LAYERS: Finding patterns in faces\n",
        "        # =================================================================\n",
        "\n",
        "        # First convolutional block - detects basic features (edges, lines)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            1, 32, kernel_size=3, padding=1\n",
        "        )  # 1 input channel (grayscale), 32 output channels\n",
        "        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization for stable training\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # Reduce image size by half\n",
        "        self.dropout1 = nn.Dropout2d(0.25)  # Prevent overfitting\n",
        "\n",
        "        # Second convolutional block - detects more complex features\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            32, 64, kernel_size=3, padding=1\n",
        "        )  # 32 input, 64 output channels\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout2 = nn.Dropout2d(0.25)\n",
        "\n",
        "        # Third convolutional block - detects high-level features (eye shapes, mouth curves)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            64, 128, kernel_size=3, padding=1\n",
        "        )  # 64 input, 128 output channels\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout3 = nn.Dropout2d(0.25)\n",
        "\n",
        "        # =================================================================\n",
        "        # CLASSIFICATION LAYERS: Making the final emotion prediction\n",
        "        # =================================================================\n",
        "\n",
        "        # Calculate the size after all pooling operations\n",
        "        # Original size: 80x80, after 3 pooling operations: 80/8 = 10x10\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Dense layers for final classification\n",
        "        self.fc1 = nn.Linear(128 * 10 * 10, 512)  # First fully connected layer\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 256)  # Second fully connected layer\n",
        "        self.dropout5 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(\n",
        "            256, num_classes\n",
        "        )  # Final layer - outputs emotion probabilities\n",
        "\n",
        "        print(f\"üß† Created CNN with {num_classes} emotion classes\")\n",
        "        print(f\"üìö Model will learn to recognize: {num_classes} different emotions\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass: How data flows through our network\n",
        "        This function defines how an image becomes an emotion prediction\n",
        "        \"\"\"\n",
        "\n",
        "        # First convolutional block\n",
        "        x = F.relu(\n",
        "            self.bn1(self.conv1(x))\n",
        "        )  # Apply convolution, normalization, and ReLU activation\n",
        "        x = self.pool1(x)  # Reduce size\n",
        "        x = self.dropout1(x)  # Apply dropout\n",
        "\n",
        "        # Second convolutional block\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Third convolutional block\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        # Flatten for dense layers\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Dense layers for classification\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout4(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout5(x)\n",
        "        x = self.fc3(\n",
        "            x\n",
        "        )  # Final layer - no activation (softmax applied in loss function)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Training step: What happens during each training iteration\n",
        "        \"\"\"\n",
        "        images, labels = batch\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = self(images)\n",
        "\n",
        "        # Calculate loss (how wrong our predictions are)\n",
        "        if self.class_weights is not None:\n",
        "            # Use weighted loss to handle class imbalance\n",
        "            criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        else:\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        accuracy = (predicted == labels).float().mean()\n",
        "\n",
        "        # Log metrics for monitoring\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_acc\", accuracy, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Validation step: Testing our model on unseen data\n",
        "        \"\"\"\n",
        "        images, labels = batch\n",
        "        outputs = self(images)\n",
        "\n",
        "        # Calculate validation loss\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        accuracy = (predicted == labels).float().mean()\n",
        "\n",
        "        # Log validation metrics\n",
        "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"val_acc\", accuracy, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return {\n",
        "            \"val_loss\": loss,\n",
        "            \"val_acc\": accuracy,\n",
        "            \"predictions\": predicted,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configure the optimizer: How our model learns and improves\n",
        "        Adam optimizer is great for image recognition tasks!\n",
        "        \"\"\"\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Learning rate scheduler - reduces learning rate when training plateaus\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode=\"min\", factor=0.5, patience=3, verbose=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"},\n",
        "        }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CREATE OUR MODEL INSTANCE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üèóÔ∏è  Building our emotion recognition model...\")\n",
        "\n",
        "# Calculate class weights to handle imbalanced data\n",
        "train_loader = data_module.train_dataloader()\n",
        "all_labels = []\n",
        "for _, labels in train_loader:\n",
        "    all_labels.extend(labels.numpy())\n",
        "\n",
        "# Compute class weights using sklearn\n",
        "class_weights_sklearn = class_weight.compute_class_weight(\n",
        "    \"balanced\", classes=np.unique(all_labels), y=all_labels\n",
        ")\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "class_weights_tensor = torch.FloatTensor(class_weights_sklearn)\n",
        "\n",
        "print(f\"‚öñÔ∏è  Class weights (to handle imbalanced data): {class_weights_sklearn}\")\n",
        "\n",
        "# Create our model\n",
        "model = EmotionCNN(\n",
        "    num_classes=data_module.num_classes,\n",
        "    learning_rate=0.001,\n",
        "    class_weights=class_weights_tensor,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model created successfully!\")\n",
        "print(f\"üéØ Ready to train on {data_module.num_classes} emotion classes\")\n",
        "\n",
        "# Let's see our model architecture\n",
        "print(\"\\nüèóÔ∏è  Model Architecture Overview:\")\n",
        "print(\"=\" * 50)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjDb-jtzI79S",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING SETUP: Preparing to teach our AI to recognize emotions\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üöÄ Setting up training configuration...\")\n",
        "\n",
        "# =================================================================\n",
        "# TRAINING CALLBACKS: Helpful tools during training\n",
        "# =================================================================\n",
        "\n",
        "# Early Stopping: Prevents overfitting by stopping when validation stops improving\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor=\"val_loss\",  # Watch validation loss\n",
        "    min_delta=0.001,  # Minimum change to qualify as improvement\n",
        "    patience=5,  # Wait 5 epochs before stopping\n",
        "    verbose=True,  # Print messages\n",
        "    mode=\"min\",  # We want to minimize loss\n",
        ")\n",
        "\n",
        "# Model Checkpoint: Saves the best version of our model\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=\"checkpoints/\",\n",
        "    filename=\"emotion-cnn-{epoch:02d}-{val_acc:.3f}\",\n",
        "    monitor=\"val_acc\",  # Save based on validation accuracy\n",
        "    mode=\"max\",  # We want to maximize accuracy\n",
        "    save_top_k=1,  # Keep only the best model\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# =================================================================\n",
        "# PYTORCH LIGHTNING TRAINER: The training orchestrator\n",
        "# =================================================================\n",
        "\n",
        "# Create the trainer - this handles all the complex training logic\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=25,  # Maximum training epochs\n",
        "    callbacks=[early_stop_callback, checkpoint_callback],  # Add our callbacks\n",
        "    logger=TensorBoardLogger(\"lightning_logs\", name=\"emotion_cnn\"),  # Logging\n",
        "    accelerator=\"auto\",  # Automatically use GPU if available\n",
        "    devices=\"auto\",  # Use available devices\n",
        "    precision=16,  # Use 16-bit precision for faster training\n",
        "    log_every_n_steps=10,  # Log metrics every 10 steps\n",
        "    deterministic=True,  # Reproducible results\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training setup complete!\")\n",
        "print(f\"üèãÔ∏è  Trainer configured for maximum {trainer.max_epochs} epochs\")\n",
        "print(f\"üîÑ Early stopping patience: {early_stop_callback.patience} epochs\")\n",
        "print(f\"üíæ Best models will be saved to: checkpoints/\")\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU acceleration available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\"   Training will be much faster!\")\n",
        "else:\n",
        "    print(\"üíª Training on CPU - this might take longer\")\n",
        "    print(\"   Consider using Google Colab with GPU for faster training\")\n",
        "\n",
        "print(\"\\nüéì What happens during training?\")\n",
        "print(\"   1. Model looks at emotion images\")\n",
        "print(\"   2. Makes predictions about emotions\")\n",
        "print(\"   3. Compares predictions to correct answers\")\n",
        "print(\"   4. Adjusts its internal parameters to improve\")\n",
        "print(\"   5. Repeats this process thousands of times!\")\n",
        "print(\"   6. Gradually gets better at recognizing emotions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t07Yf2EcI79S",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING TIME: Teaching our AI to recognize emotions! üß†\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üéØ Starting emotion recognition training...\")\n",
        "print(\"This is where the magic happens - our AI learns to read emotions!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Start training - PyTorch Lightning handles all the complexity for us!\n",
        "trainer.fit(model, data_module)\n",
        "\n",
        "print(\"\\nüéâ Training complete!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Training results summary\n",
        "print(f\"‚úÖ Best model saved at: {checkpoint_callback.best_model_path}\")\n",
        "print(f\"üèÜ Best validation accuracy: {checkpoint_callback.best_model_score:.3f}\")\n",
        "\n",
        "print(\"\\nüß† What did our AI learn?\")\n",
        "print(\"   - How to detect facial features (eyes, mouth, eyebrows)\")\n",
        "print(\"   - Patterns that indicate different emotions\")\n",
        "print(\"   - How to distinguish between happy, sad, angry, etc.\")\n",
        "print(\"   - The ability to recognize emotions in new faces it hasn't seen before!\")\n",
        "\n",
        "print(\"\\nüìä Next steps:\")\n",
        "print(\"   - Evaluate model performance on test data\")\n",
        "print(\"   - Visualize what the model learned\")\n",
        "print(\"   - Test it on new images\")\n",
        "print(\"   - Use it in our game for interactive NPCs!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o4e6FcuI79T",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL EVALUATION: How well did our AI learn to recognize emotions?\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üìä Evaluating our emotion recognition model...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test the model on validation data\n",
        "test_results = trainer.test(model, data_module.val_dataloader())\n",
        "\n",
        "print(f\"üéØ Final Test Results:\")\n",
        "print(f\"   Test Accuracy: {test_results[0]['test_acc']:.3f}\")\n",
        "print(f\"   Test Loss: {test_results[0]['test_loss']:.3f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING HISTORY VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def plot_training_history(trainer):\n",
        "    \"\"\"\n",
        "    Visualize how our model improved during training\n",
        "    This helps us understand if our model learned properly\n",
        "    \"\"\"\n",
        "\n",
        "    # Get metrics from the logger\n",
        "    metrics = trainer.logger.experiment\n",
        "\n",
        "    # Try to get training history from the trainer\n",
        "    if hasattr(trainer, \"logged_metrics\"):\n",
        "        print(\"üìà Creating training history plots...\")\n",
        "\n",
        "        # Create a figure with two subplots\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Note: In a real scenario, you'd extract metrics from the logger\n",
        "        # For now, we'll create example plots to show the concept\n",
        "\n",
        "        # Accuracy plot\n",
        "        ax1.set_title(\"Model Accuracy Over Time\", fontweight=\"bold\", fontsize=14)\n",
        "        ax1.set_xlabel(\"Epoch\")\n",
        "        ax1.set_ylabel(\"Accuracy\")\n",
        "        ax1.text(\n",
        "            0.5,\n",
        "            0.5,\n",
        "            \"üìä Training accuracy plot\\nwould appear here\\n(extracted from logger)\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            transform=ax1.transAxes,\n",
        "            fontsize=12,\n",
        "        )\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss plot\n",
        "        ax2.set_title(\"Model Loss Over Time\", fontweight=\"bold\", fontsize=14)\n",
        "        ax2.set_xlabel(\"Epoch\")\n",
        "        ax2.set_ylabel(\"Loss\")\n",
        "        ax2.text(\n",
        "            0.5,\n",
        "            0.5,\n",
        "            \"üìâ Training loss plot\\nwould appear here\\n(extracted from logger)\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            transform=ax2.transAxes,\n",
        "            fontsize=12,\n",
        "        )\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"üìà What these plots tell us:\")\n",
        "        print(\"   - Accuracy should increase over time (model getting better)\")\n",
        "        print(\"   - Loss should decrease over time (model making fewer mistakes)\")\n",
        "        print(\"   - Training and validation curves should be close (no overfitting)\")\n",
        "    else:\n",
        "        print(\"üìä Training history plots would be generated from logged metrics\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CONFUSION MATRIX: Detailed performance analysis\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def create_confusion_matrix(model, data_loader, class_names):\n",
        "    \"\"\"\n",
        "    Create a confusion matrix to see which emotions our model confuses\n",
        "    This is super helpful for understanding model strengths and weaknesses!\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üîç Creating confusion matrix...\")\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Collect predictions and true labels\n",
        "    with torch.no_grad():  # No gradient computation needed for evaluation\n",
        "        for images, labels in data_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "    plt.title(\"Emotion Recognition Confusion Matrix\", fontweight=\"bold\", fontsize=16)\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Add labels\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    # Add text annotations\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(\n",
        "            j,\n",
        "            i,\n",
        "            format(cm[i, j], \"d\"),\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "            fontweight=\"bold\",\n",
        "        )\n",
        "\n",
        "    plt.ylabel(\"True Emotion\", fontweight=\"bold\")\n",
        "    plt.xlabel(\"Predicted Emotion\", fontweight=\"bold\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed analysis\n",
        "    print(\"\\nüîç Confusion Matrix Analysis:\")\n",
        "    print(\"   - Diagonal values = correct predictions\")\n",
        "    print(\"   - Off-diagonal values = mistakes\")\n",
        "    print(\"   - Higher diagonal values = better performance\")\n",
        "\n",
        "    # Calculate per-class accuracy\n",
        "    class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
        "    print(f\"\\nüìä Per-emotion accuracy:\")\n",
        "    for emotion, acc in zip(class_names, class_accuracies):\n",
        "        print(f\"   {emotion}: {acc:.3f} ({acc*100:.1f}%)\")\n",
        "\n",
        "    return cm\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SAMPLE PREDICTIONS: See our model in action!\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def visualize_predictions(model, data_loader, class_names, num_samples=8):\n",
        "    \"\"\"\n",
        "    Show sample predictions to see how our model performs on real images\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üñºÔ∏è  Sample Predictions from Our Model:\")\n",
        "\n",
        "    model.eval()\n",
        "    images_shown = 0\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    fig.suptitle(\"Sample Emotion Predictions\", fontsize=16, fontweight=\"bold\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            outputs = model(images)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            for i in range(min(len(images), num_samples - images_shown)):\n",
        "                row = images_shown // 4\n",
        "                col = images_shown % 4\n",
        "\n",
        "                # Display image\n",
        "                img = images[i].squeeze().cpu().numpy()\n",
        "                axes[row, col].imshow(img, cmap=\"gray\")\n",
        "\n",
        "                # Get prediction info\n",
        "                true_emotion = class_names[labels[i]]\n",
        "                pred_emotion = class_names[predicted[i]]\n",
        "                confidence = probabilities[i][predicted[i]].item()\n",
        "\n",
        "                # Set title with prediction\n",
        "                color = \"green\" if true_emotion == pred_emotion else \"red\"\n",
        "                title = f\"True: {true_emotion}\\nPred: {pred_emotion}\\nConf: {confidence:.2f}\"\n",
        "                axes[row, col].set_title(title, color=color, fontweight=\"bold\")\n",
        "                axes[row, col].axis(\"off\")\n",
        "\n",
        "                images_shown += 1\n",
        "                if images_shown >= num_samples:\n",
        "                    break\n",
        "\n",
        "            if images_shown >= num_samples:\n",
        "                break\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"üéØ Prediction Legend:\")\n",
        "    print(\"   üü¢ Green titles = Correct predictions\")\n",
        "    print(\"   üî¥ Red titles = Incorrect predictions\")\n",
        "    print(\"   Conf = Model's confidence (0.0 to 1.0)\")\n",
        "\n",
        "\n",
        "# Run all evaluations\n",
        "print(\"üöÄ Running comprehensive model evaluation...\")\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(trainer)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = create_confusion_matrix(\n",
        "    model, data_module.val_dataloader(), data_module.class_names\n",
        ")\n",
        "\n",
        "# Show sample predictions\n",
        "visualize_predictions(model, data_module.val_dataloader(), data_module.class_names)\n",
        "\n",
        "print(\"\\nüéâ Evaluation complete!\")\n",
        "print(\"üìù Key takeaways:\")\n",
        "print(\"   - Check confusion matrix for emotion pairs the model confuses\")\n",
        "print(\"   - Look at sample predictions to see real performance\")\n",
        "print(\"   - High confidence correct predictions = good learning\")\n",
        "print(\"   - Use this analysis to improve the model further\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHaZCvxpRsga"
      },
      "source": [
        "## Save our model\n",
        "\n",
        "Now that we've trained our model, we can save it for the next steps where we will want to use the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdDCs9OlI79T"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SAVE OUR TRAINED MODEL: Preserving our AI's learned knowledge\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üíæ Saving our trained emotion recognition model...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# The best model was automatically saved during training by our checkpoint callback\n",
        "best_model_path = checkpoint_callback.best_model_path\n",
        "print(f\"üèÜ Best model automatically saved at: {best_model_path}\")\n",
        "\n",
        "# Let's also save the model in a format that's easy to use later\n",
        "# This creates a complete package with both the model architecture and learned weights\n",
        "\n",
        "# Save the entire model (architecture + weights)\n",
        "torch.save(\n",
        "    {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"class_names\": data_module.class_names,\n",
        "        \"num_classes\": data_module.num_classes,\n",
        "        \"model_config\": {\n",
        "            \"learning_rate\": model.learning_rate,\n",
        "            \"input_size\": (80, 80),\n",
        "        },\n",
        "    },\n",
        "    \"emotion_recognition_model.pth\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model saved as 'emotion_recognition_model.pth'\")\n",
        "\n",
        "# Also save in ONNX format for broader compatibility (optional)\n",
        "try:\n",
        "    # Export to ONNX format (works with many different frameworks)\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(1, 1, 80, 80)  # Example input tensor\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        \"emotion_model.onnx\",\n",
        "        export_params=True,\n",
        "        opset_version=11,\n",
        "        do_constant_folding=True,\n",
        "        input_names=[\"input\"],\n",
        "        output_names=[\"output\"],\n",
        "    )\n",
        "    print(\"‚úÖ Model also saved in ONNX format as 'emotion_model.onnx'\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  ONNX export failed: {e}\")\n",
        "\n",
        "print(\"\\nüìÅ Saved Files Summary:\")\n",
        "print(f\"   üèÜ Best checkpoint: {best_model_path}\")\n",
        "print(\"   üíæ Complete model: emotion_recognition_model.pth\")\n",
        "print(\"   üåê ONNX format: emotion_model.onnx (if successful)\")\n",
        "\n",
        "print(\"\\nüéÆ Ready for Game Integration!\")\n",
        "print(\"=\" * 40)\n",
        "print(\"Our emotion recognition model is now ready to be used in the game!\")\n",
        "print(\"üìã Next steps for game integration:\")\n",
        "print(\"   1. Load the saved model in the game code\")\n",
        "print(\"   2. Connect webcam/camera input\")\n",
        "print(\"   3. Process player's facial expressions in real-time\")\n",
        "print(\"   4. Use emotion predictions to influence NPC dialogue\")\n",
        "print(\"   5. Create dynamic, emotion-responsive gameplay!\")\n",
        "\n",
        "print(\"\\nüß† What our AI has learned:\")\n",
        "print(\"   ‚úì Recognize facial features and expressions\")\n",
        "print(\"   ‚úì Classify emotions with high accuracy\")\n",
        "print(\"   ‚úì Handle real-world variations in lighting and faces\")\n",
        "print(\"   ‚úì Make predictions in real-time\")\n",
        "\n",
        "print(\"\\nüéØ Model Performance Summary:\")\n",
        "print(f\"   üìä Training completed successfully\")\n",
        "print(f\"   üéØ Model can recognize {data_module.num_classes} different emotions\")\n",
        "print(f\"   üìà Ready for real-world emotion detection!\")\n",
        "\n",
        "print(\"\\nüöÄ Congratulations! You've built an AI emotion recognition system!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nldt4Ez1UIjl"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PRACTICAL DEMO: Using our model for real-time emotion recognition\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def load_trained_model(model_path=\"emotion_recognition_model.pth\"):\n",
        "    \"\"\"\n",
        "    Load our saved emotion recognition model\n",
        "    This shows how we'll use the model in our game!\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üîÑ Loading trained emotion recognition model...\")\n",
        "\n",
        "    # Load the saved model data\n",
        "    checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
        "\n",
        "    # Recreate the model architecture\n",
        "    loaded_model = EmotionCNN(\n",
        "        num_classes=checkpoint[\"num_classes\"],\n",
        "        learning_rate=checkpoint[\"model_config\"][\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "    # Load the learned weights\n",
        "    loaded_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    loaded_model.eval()  # Set to evaluation mode\n",
        "\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "    return loaded_model, checkpoint[\"class_names\"]\n",
        "\n",
        "\n",
        "def predict_emotion_from_image(model, image_array, class_names):\n",
        "    \"\"\"\n",
        "    Predict emotion from a single image\n",
        "    This is how our game will analyze player expressions!\n",
        "    \"\"\"\n",
        "\n",
        "    # Preprocess the image (same as training)\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.Resize((80, 80)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Convert image and add batch dimension\n",
        "    if isinstance(image_array, np.ndarray):\n",
        "        # Convert numpy array to PIL Image\n",
        "        from PIL import Image\n",
        "\n",
        "        if len(image_array.shape) == 3:\n",
        "            image_array = np.mean(image_array, axis=2)  # Convert to grayscale\n",
        "        pil_image = Image.fromarray((image_array * 255).astype(np.uint8))\n",
        "        tensor_image = transform(pil_image).unsqueeze(0)\n",
        "    else:\n",
        "        tensor_image = transform(image_array).unsqueeze(0)\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tensor_image)\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        confidence, predicted_idx = torch.max(probabilities, 1)\n",
        "\n",
        "        predicted_emotion = class_names[predicted_idx.item()]\n",
        "        confidence_score = confidence.item()\n",
        "\n",
        "    return predicted_emotion, confidence_score, probabilities[0].numpy()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# GAME INTEGRATION EXAMPLE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üéÆ GAME INTEGRATION EXAMPLE\")\n",
        "print(\"=\" * 40)\n",
        "print(\"Here's how our emotion recognition will work in the game:\")\n",
        "\n",
        "# Simulate loading the model (as it would happen in the game)\n",
        "try:\n",
        "    if os.path.exists(\"emotion_recognition_model.pth\"):\n",
        "        game_model, game_class_names = load_trained_model()\n",
        "\n",
        "        print(f\"üéØ Model ready with emotions: {game_class_names}\")\n",
        "\n",
        "        # Example of how the game would use this\n",
        "        print(\"\\nüéÆ Game Integration Workflow:\")\n",
        "        print(\"   1. üì∑ Capture player's face from webcam\")\n",
        "        print(\"   2. üñºÔ∏è  Preprocess image (resize, normalize)\")\n",
        "        print(\"   3. üß† Run through emotion recognition model\")\n",
        "        print(\"   4. üé≠ Get emotion prediction + confidence\")\n",
        "        print(\"   5. üí¨ Adjust NPC dialogue based on emotion\")\n",
        "        print(\"   6. üîÑ Repeat in real-time during gameplay\")\n",
        "\n",
        "        # Simulate game dialogue logic\n",
        "        def generate_npc_response(detected_emotion, confidence):\n",
        "            \"\"\"\n",
        "            Example of how NPCs might respond to player emotions\n",
        "            \"\"\"\n",
        "            responses = {\n",
        "                \"happy\": [\n",
        "                    \"Great to see you smiling! Let me help you with something special.\",\n",
        "                    \"Your happiness is contagious! Here's a bonus for you.\",\n",
        "                ],\n",
        "                \"sad\": [\n",
        "                    \"I can see you're feeling down. Is there anything I can do to help?\",\n",
        "                    \"Don't worry, things will get better. Here's something to cheer you up.\",\n",
        "                ],\n",
        "                \"angry\": [\n",
        "                    \"I can see you're frustrated. Let's work together to solve this.\",\n",
        "                    \"Take a deep breath. I'm here to help, not make things worse.\",\n",
        "                ],\n",
        "                \"surprise\": [\n",
        "                    \"Oh! You look surprised! Did something unexpected happen?\",\n",
        "                    \"That's quite a reaction! What caught you off guard?\",\n",
        "                ],\n",
        "                \"fear\": [\n",
        "                    \"You seem worried. Don't be afraid, I'm here to help you.\",\n",
        "                    \"Is something troubling you? Let's face it together.\",\n",
        "                ],\n",
        "                \"neutral\": [\n",
        "                    \"Hello there! How can I assist you today?\",\n",
        "                    \"Welcome! What brings you to see me?\",\n",
        "                ],\n",
        "            }\n",
        "\n",
        "            if confidence > 0.7:  # High confidence\n",
        "                return f\"[CONFIDENT] {responses.get(detected_emotion, responses['neutral'])[0]}\"\n",
        "            else:  # Lower confidence\n",
        "                return f\"[UNCERTAIN] {responses.get('neutral')[0]}\"\n",
        "\n",
        "        # Example usage\n",
        "        print(\"\\nüé≠ Example NPC Responses:\")\n",
        "        for emotion in game_class_names:\n",
        "            response = generate_npc_response(emotion, 0.85)\n",
        "            print(f\"   {emotion.upper()}: {response}\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Model file not found. Train the model first!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "\n",
        "print(\"\\nüöÄ NEXT STEPS FOR STUDENTS:\")\n",
        "print(\"=\" * 30)\n",
        "print(\"1. üß™ Experiment with different emotions in front of a camera\")\n",
        "print(\"2. üéÆ Integrate this model into the provided game framework\")\n",
        "print(\"3. üí¨ Create more sophisticated NPC dialogue trees\")\n",
        "print(\"4. üé® Add visual feedback when emotions are detected\")\n",
        "print(\"5. üìä Collect data on how players respond to emotion-aware NPCs\")\n",
        "\n",
        "print(\"\\nüéì LEARNING OBJECTIVES ACHIEVED:\")\n",
        "print(\"=\" * 35)\n",
        "print(\"‚úÖ Understanding of Computer Vision concepts\")\n",
        "print(\"‚úÖ Hands-on experience with Deep Learning\")\n",
        "print(\"‚úÖ Practical application of AI in gaming\")\n",
        "print(\"‚úÖ Real-time emotion recognition system\")\n",
        "print(\"‚úÖ Integration of AI with interactive applications\")\n",
        "\n",
        "print(\"\\nüåü CONGRATULATIONS!\")\n",
        "print(\"You've successfully built an AI system that can:\")\n",
        "print(\"   üß† Understand human emotions from facial expressions\")\n",
        "print(\"   üéÆ Enhance gaming experiences with emotional intelligence\")\n",
        "print(\"   üî¨ Apply cutting-edge computer vision technology\")\n",
        "print(\"   üöÄ Create more engaging and responsive applications\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéä WELCOME TO THE FUTURE OF EMOTION-AWARE TECHNOLOGY! üéä\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
